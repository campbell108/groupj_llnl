{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db801cc-8523-4d40-b046-0e3bca625199",
   "metadata": {},
   "source": [
    "# DSC25 resources\n",
    "\n",
    "Classes and functions provided by Amar for the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09cdff-ca47-4f46-9e84-aa0f6b91d1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cebcd8f-01d5-49f7-9ad9-1567d04d4127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch, Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import write_video\n",
    "\n",
    "# Common\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from IPython.display import Video\n",
    "\n",
    "# Utils from Torchvision\n",
    "tensor_to_image = ToPILImage()\n",
    "image_to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c6cb5-f3b4-4603-bff6-06e4865639a1",
   "metadata": {},
   "source": [
    "# 2025 DSC Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23dd63-b781-4572-be3c-9ed640050663",
   "metadata": {},
   "source": [
    "Below are 4 tasks!\n",
    "\n",
    "The goal here is to\n",
    "- (1) First train models to solve the 4 tasks *quantiatively*\n",
    "- (2) *Qualitatively* evaluate your trained models on real-world data, specifically on the <u>**Robotics Laboratory Pick and Place Dataset**</u>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c7739-cb82-46b9-a017-357247048457",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b68dfd-d162-400a-a89e-585a2104d3f6",
   "metadata": {},
   "source": [
    "### Task 1.1: (Image-based) Modal Mask -> Amodal Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e049a2-a4ce-4acc-8c18-547eb1e6111b",
   "metadata": {},
   "source": [
    "> \"Given an image of the modal mask of an object, predict the amodal mask of the same object\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286fc17-a72b-4e36-a384-d04134ead1c8",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- RGB Frame\n",
    "  - 3-channel image (3, 256, 256)\n",
    "- Modal Mask of Object *i*\n",
    "  - Binary (1-channel) image (1, 256, 256)\n",
    "\n",
    "Outputs:\n",
    "- Amodal Mask of Object *i*\n",
    "  - Binary (1-channel) image (1, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2aeea-7315-48be-af1d-061f34c2b2d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0c3d4-b888-46d6-baeb-2a062e93778d",
   "metadata": {},
   "source": [
    "### Task 1.2: (Image-based) Modal Content (RGB) -> Amodal Content (RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470d737-46c6-4b03-b8f5-37929a770523",
   "metadata": {},
   "source": [
    "> \"Given an image of the modal RGB content of an object, predict the amodal RGB content of the same object\"\n",
    "\n",
    "> \"Can use the amodal masks of that object\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8a5c8-e99d-4bd7-a8f4-f820c5120ed0",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- RGB Frame\n",
    "  - 3-channel image (3, 256, 256)\n",
    "- Amodal Mask of Object *i*\n",
    "  - Binary (1-channel) image (1, 256, 256)\n",
    "- RGB Content of Object *i*\n",
    "  - 3-channel image (3, 256, 256)\n",
    "  - Use the object's modal mask to \"crop out\" the RGB content of object *i*\n",
    "  - Optional to use?\n",
    "  \n",
    "Outputs:\n",
    "- Amodal RGB Content of Object *i*\n",
    "  - 3-channel image (3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40769bc2-a2a4-4a18-af8b-03246b944f1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc661a21-e01b-49f4-8b54-4feb2903d18c",
   "metadata": {},
   "source": [
    "### Task 2.1: (Video-based) Modal Mask -> Amodal Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3015631-3d7f-47a5-948a-46ce5be3dc29",
   "metadata": {},
   "source": [
    "> \"Given a video of the modal mask of an object, predict the amodal mask of the same object\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e577389-a3ce-4a76-953d-e50771f2c94f",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- RGB Frames\n",
    "  - N 3-channel images (N, 3, 256, 256)\n",
    "- N Modal Masks of Object *i*\n",
    "  - N Binary (1-channel) images (N, 1, 256, 256)\n",
    "\n",
    "Outputs:\n",
    "- N Amodal Masks of Object *i*\n",
    "  - N Binary (1-channel) images (1, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fcc63-9cc5-4ab1-a01d-7bd1fb13f496",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47194926-91c4-4429-9bc7-5a69a24da057",
   "metadata": {},
   "source": [
    "### Task 2.2: (Video-based) Modal Content (RGB) -> Amodal Content (RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927ae2c-e49d-43ff-a133-2bc49b618163",
   "metadata": {},
   "source": [
    "> \"Given a video of the modal RGB content of an object, predict the amodal RGB content of the same object\"\n",
    "\n",
    "> \"Can use the amodal masks of that object\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131e077-5db1-4670-91b4-e634dc846672",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- N RGB Frames\n",
    "  - N 3-channel images (N, 3, 256, 256)\n",
    "- N Amodal Masks of Object *i*\n",
    "  - N Binary (1-channel) images (N, 1, 256, 256)\n",
    "- N RGB Contents of Object *i*\n",
    "  - N 3-channel images (N, 3, 256, 256)\n",
    "  - Use the object's modal mask to \"crop out\" the RGB content of object *i*\n",
    "  - Optional to use?\n",
    "  \n",
    "Outputs:\n",
    "- N Amodal RGB Contents of Object *i*\n",
    "  - N 3-channel images (N, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d21fb-75c9-4a1e-bd42-f791be99528b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa2243-0cfc-402e-85a2-9aa49d5c5828",
   "metadata": {},
   "source": [
    "### Bonus Task 3: Create Modal Masks with SAM2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73793ce-b0f4-4d99-a928-8d16473e792b",
   "metadata": {},
   "source": [
    "> \"Run SAM2 on the <u>**Robotics Laboratory Pick and Place Dataset**</u> and make your own modal masks!\"\n",
    "- https://github.com/facebookresearch/sam2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03209860-6d32-464e-8772-83642e1542cc",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- Video\n",
    "- For the object of interest: clickable points, bounding boxes, masks\n",
    "\n",
    "Output:\n",
    "- Masklet (Object masks across the video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f3851-a6b1-4395-9120-3113b8f9294f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e346c6-51d7-4399-a31e-52cc544c1e50",
   "metadata": {},
   "source": [
    "### Bonus Task 4: Re-ID of Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396bb5a-04e9-4b6e-964e-78df5aa79f40",
   "metadata": {},
   "source": [
    "> \"Given 2 objects, are they the same?\"\n",
    "\n",
    "> \"Given an object, retrieve similar objects from a catalog of objects\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70eb7-404e-4c85-a4b0-8b746ab8344a",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- Image or Video data?\n",
    "- Modal or Amodal data?\n",
    "- Masks, RGB Content?\n",
    "  \n",
    "Outputs:\n",
    "- Are the 2 objects the same?\n",
    "- Retrieved similar objects A, B, C, given object X?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb5ca5-6e30-434b-a685-27e89f65e7ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01527fe-c02a-4c2f-a605-364c98cb1fce",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c0999-3795-432b-9929-eea0f36f2158",
   "metadata": {},
   "source": [
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa4bf0-2b37-4882-8da4-f6fbb9c822c8",
   "metadata": {},
   "source": [
    "# More advanced examples/extra material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aeb44a-a872-4af6-aa3b-1a14910a9420",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfa1bd-1336-42df-aa81-4f80dd8d8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(amodal_mask_preds, \n",
    "                      amodal_mask_labels, \n",
    "                      amodal_content_preds,\n",
    "                      amodal_content_labels):\n",
    "    \"\"\"\n",
    "    Here, you can calculate non-loss metrics like mIOU, accuracy, J&F scores.\n",
    "\n",
    "    And non-loss image generation metrics between the predicted and ground-truth amodal content\n",
    "    Such as Inception Score, Frechet Inception Distance, Learned Perceptual Patch Similarity (LPIPS),\n",
    "    Structure Similarity Index Metric (SSIM), Peak Signal-Noise Ratio (PSNR)\n",
    "\n",
    "    These should all have easy-to-use implementations in libraries such as TorchMetrics.\n",
    "    \"\"\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e4edf-2b65-4ecc-8ff0-90e2fd392b3d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b5945-b354-46ff-bf48-93740018fccd",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23e6b2-1683-4fd4-ae07-f357def848c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MOVi_Dataset(Dataset):\n",
    "    def __init__(self, split = 'train' or 'test', \n",
    "                 n_frames = 8,\n",
    "                 n_samples = 1000,\n",
    "                 #box_format = 'xywh'\n",
    "                 ):\n",
    "        print('Dataset init on', split)\n",
    "\n",
    "        self.split = split\n",
    "        self.top_dir = f'/data1/Video/CVP/data/MOVi-MC-AC/{split}/'\n",
    "        print('Init data top dir:', self.top_dir)\n",
    "\n",
    "        #self.box_format = box_format\n",
    "\n",
    "        # Get directories in data_dir/train-test\n",
    "        self.scenes = [entry for entry in os.listdir(self.top_dir) if os.path.isdir(os.path.join(self.top_dir, entry))]\n",
    "\n",
    "        self.n_frames = n_frames\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        # In theory this could be like n_scenes*n_objects\n",
    "        # To get total number of (cam-invariant) objects\n",
    "        return self.n_samples\n",
    "\n",
    "    def load_cam_frames(self, scene, \n",
    "                    cam_idx,\n",
    "                    start, stop, \n",
    "                    modality):\n",
    "        \"\"\"\n",
    "        One load-frames loads camera-level stuff (rgb, depth)\n",
    "        The other one loads object-level stuff (scene/cam/obj_i/amodal_mask or content)\n",
    "        \"\"\"\n",
    "        # Load frame range\n",
    "        imgs = []\n",
    "        suffix = '.png'\n",
    "\n",
    "        totensor = ToTensor()\n",
    "\n",
    "        for i in range(start, stop):\n",
    "            # loads train/scene_id/cam_id/frames_or_depth_or_modal/frame_id.png\n",
    "            if modality == 'modal_masks':\n",
    "                load_file = f'{self.top_dir}/{scene}/{cam_idx}/segmentation_{str(i).zfill(5)}{suffix}'\n",
    "            \n",
    "            if modality == 'rgba_full':\n",
    "                load_file = f'{self.top_dir}/{scene}/{cam_idx}/rgba_{str(i).zfill(5)}{suffix}'\n",
    "\n",
    "            if modality == 'depth_full':\n",
    "                load_file = f'{self.top_dir}/{scene}/{cam_idx}/depth_{str(i).zfill(5)}.tiff'\n",
    "\n",
    "            tens = totensor(Image.open(load_file))\n",
    "            imgs.append(tens)\n",
    "\n",
    "        tensor = torch.stack(imgs, dim = 1)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def load_obj_frames(self, scene, \n",
    "                    cam_idx,\n",
    "                    object_idx,\n",
    "                    start, stop, \n",
    "                    modality):\n",
    "        \"\"\"\n",
    "        This loaded loads object-level stuff\n",
    "        \"\"\"\n",
    "        # Load frame range\n",
    "        imgs = []\n",
    "        # amodal_segs, content, rgba_full, modal_masks, depth_full\n",
    "        suffix = '.png'\n",
    "\n",
    "        totensor = ToTensor()\n",
    "\n",
    "        for i in range(start, stop):\n",
    "            if modality == 'amodal_segs':\n",
    "                load_file = f'{self.top_dir}/{scene}/{cam_idx}/{object_idx}/segmentation_{str(i).zfill(5)}{suffix}'\n",
    "                tens = totensor(Image.open(load_file))\n",
    "\n",
    "            if modality == 'content':\n",
    "                load_file = f'{self.top_dir}/{scene}/{cam_idx}/{object_idx}/rgba_{str(i).zfill(5)}{suffix}'\n",
    "                tens = totensor(Image.open(load_file).convert('RGB'))\n",
    "                \n",
    "            if modality == 'depth_full':\n",
    "                load_file = f'{self.top_dir}/{scene}/{cam_idx}/{object_idx}/rgba_{str(i).zfill(5)}{suffix}'\n",
    "                tens = totensor(Image.open(load_file).convert('RGB'))\n",
    "            imgs.append(tens)\n",
    "\n",
    "        tensor = torch.stack(imgs, dim = 1)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select a random sample\n",
    "        random_scene = np.random.choice(self.scenes)\n",
    "\n",
    "        # Get the list of objects in that sample\n",
    "        all_object_ids = self.all_objects(self.top_dir + random_scene + '/camera_0000/' )\n",
    "        \n",
    "        # Pick a random object \n",
    "        target_object_id = np.random.choice(all_object_ids)\n",
    "\n",
    "        \"\"\"\n",
    "        Loading from multiple cameras in parallel:\n",
    "        \"\"\"\n",
    "\n",
    "        # Make these random\n",
    "        start = random.randint(0, 24-self.n_frames)\n",
    "        stop = start+self.n_frames\n",
    "\n",
    "        i = random.randint(0, 5)\n",
    "        frames, depths, modal_masks, amodal_segs, amodal_content = self.load_camera(random_scene, cam_id = f'camera_{str(i).zfill(4)}', obj_id = target_object_id, start = start, stop = stop)\n",
    "\n",
    "        modal_masks = modal_masks*255\n",
    "        modal_masks = modal_masks.to(torch.uint8)\n",
    "        sample = {\n",
    "            'frames': frames,\n",
    "            'depths': depths,\n",
    "            'modal_masks': modal_masks,\n",
    "            'amodal_masks': amodal_segs,\n",
    "            'amodal_content': amodal_content,\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    \n",
    "    def load_camera(self, scene_id, cam_id, obj_id, start, stop):\n",
    "\n",
    "        # Load the target objects \n",
    "        modal_segs = self.load_cam_frames(scene_id, \n",
    "                                            cam_id,\n",
    "                                            start, stop,\n",
    "                                            'modal_masks')\n",
    "\n",
    "        modal_segs = modal_segs*255\n",
    "        modal_segs = modal_segs.int()\n",
    "\n",
    "        # Load frames corresponding to inputs\n",
    "        frames = self.load_cam_frames(scene_id, \n",
    "                                      cam_id, \n",
    "                                      start, \n",
    "                                      stop, \n",
    "                                      'rgba_full')[:-1]\n",
    "\n",
    "        # Load depth (though we will have to replace with Depth-Anything-V2 estimates)\n",
    "        depths = self.load_cam_frames(scene_id, cam_id, start, stop, 'depth_full')\n",
    "\n",
    "        amodal_segs = self.load_obj_frames(scene_id, cam_id, obj_id, start, stop, 'amodal_segs')\n",
    "        amodal_content = self.load_obj_frames(scene_id, cam_id, obj_id, start, stop, 'content')\n",
    "        \n",
    "        return frames, depths, modal_segs, amodal_segs, amodal_content\n",
    "    \n",
    "    def all_objects(self, pth):\n",
    "        \"\"\"\n",
    "        Given a path, get the objects at that path using regex\n",
    "        \"\"\"\n",
    "        #print('looking for all objects at', pth)\n",
    "        \n",
    "        # Find all matches\n",
    "        matches = []\n",
    "        for fname in sorted(os.listdir(pth)):\n",
    "            if 'obj_' in fname:\n",
    "                matches.append(fname)\n",
    "\n",
    "        #print(matches)\n",
    "        return matches # list of ['obj_0001', 'obj_0009',...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d5ec1-0d9f-48c9-8b15-9169b5f4920b",
   "metadata": {},
   "source": [
    "#### Example Usage & Visualzations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe31efd-e067-4543-a04c-47ff65775c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOVi_Dataset(split = 'train', n_frames = 1)\n",
    "sample = next(iter(dataset))\n",
    "\n",
    "print(sample.keys())\n",
    "\n",
    "for key, value in sample.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd780be-01e4-4973-ba4b-542c56cb4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = sample['depths'][0][0]\n",
    "print(x.min(), x.max())\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(sample['frames'][:, 0].permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(sample['amodal_masks'][:, 0].permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(sample['amodal_content'][:, 0].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2378d-5a4e-4e3b-9236-dd3ed91a1e3b",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10f98f-def2-405c-9337-dc4529ba75db",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d73c7-a654-4ac9-852d-31a67f44665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class conv2d_inplace_spatial(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, pooling_function, activation = nn.GELU()):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation,\n",
    "            pooling_function,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, scale_factor=(2, 2), mode='bilinear', align_corners=False):\n",
    "        super(Upscale, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
    "\n",
    "class Unet_Image(nn.Module):\n",
    "    def __init__(self, in_channels = 4, mask_content_preds = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mpool_2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.down1 = conv2d_inplace_spatial(in_channels, 32, self.mpool_2)\n",
    "        self.down2 = conv2d_inplace_spatial(32, 64, self.mpool_2)\n",
    "        self.down3 = conv2d_inplace_spatial(64, 128, self.mpool_2)\n",
    "        self.down4 = conv2d_inplace_spatial(128, 256, self.mpool_2)\n",
    "\n",
    "        self.upscale_2 = Upscale(scale_factor=(2, 2), mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.up1 = conv2d_inplace_spatial(256, 128, self.upscale_2)\n",
    "        self.up2 = conv2d_inplace_spatial(256, 64, self.upscale_2)\n",
    "        self.up3 = conv2d_inplace_spatial(128, 32, self.upscale_2)\n",
    "        \n",
    "        self.up4_amodal_mask = conv2d_inplace_spatial(64, 1, self.upscale_2, activation = nn.Identity())\n",
    "        self.up4_amodal_content = conv2d_inplace_spatial(64, 3, self.upscale_2, activation = nn.Identity())\n",
    "\n",
    "        # Optional arguments\n",
    "        self.mask_content_preds = mask_content_preds # Should we mask the amodal content prediction by the amodal mask prediction?\n",
    "\n",
    "        # Optimization\n",
    "        self.mse_loss = nn.L1Loss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "        # Self-attention feature enrichment\n",
    "        #max_seq = 16*16*6\n",
    "        #token_dim = 256\n",
    "        #self.pos_enc = nn.Parameter(torch.zeros((max_seq, 1, token_dim))) # seq b dim\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=token_dim, nhead=8)\n",
    "        #self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "\n",
    "        # [torch.Size([6, 32, 4, 128, 128]), torch.Size([6, 64, 2, 64, 64]), torch.Size([6, 128, 1, 32, 32]), torch.Size([6, 256, 1, 16, 16])]\n",
    "        return x1, x2, x3, x4\n",
    "    \n",
    "    def decode(self, h1, h2, h3, h4):\n",
    "        h4 = self.up1(h4) # 6, 256, 1, 16, 16 -> 6, 128, 1, 32, 32 (double spatial, then conv-in-place channels to half)\n",
    "        h34 = torch.cat((h3, h4), dim = 1) # (6, 2*128, 1, 32, 32)\n",
    "\n",
    "        h34 = self.up2(h34) # 6, 256, 1, 32, 32 -> 6, 128, 2, 64, 64\n",
    "        h234 = torch.cat((h2, h34), dim = 1)\n",
    "\n",
    "        h234 = self.up3(h234)\n",
    "        h1234 = torch.cat((h1, h234), dim = 1)\n",
    "        \n",
    "        logits_amodal_mask = self.up4_amodal_mask(h1234)\n",
    "        logits_amodal_content = self.up4_amodal_content(h1234)\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "    \n",
    "    def encode_decode(self, x):\n",
    "        \"\"\"\n",
    "        input image tensor: (bs, c, h, w)\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # Multiscale features x1, x2, x3, x4\n",
    "        x1, x2, x3, x4 = self.encode(x)\n",
    "\n",
    "        # You could add code here for example more layers that modify the latent x4? Be creative :)\n",
    "\n",
    "        # Decode using enriched features\n",
    "        logits_amodal_mask, logits_amodal_content = self.decode(x1, x2, x3, x4)\n",
    "\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # concat rgb and modal masks as input to model\n",
    "        model_input = torch.cat((batch['frames'], batch['modal_masks']), dim = 1)\n",
    "\n",
    "        # Remove time dimension from a few things (add it back in later when you make your video model!)\n",
    "        model_input = model_input.squeeze(2) # remove time dimension (you will probably want it later!)\n",
    "        amodal_mask_labels = batch['amodal_masks'].float().squeeze(2)\n",
    "        amodal_content_labels = batch['amodal_content'].float().squeeze(2)\n",
    "\n",
    "        # Model input\n",
    "        #print('model input:', model_input.shape)\n",
    "        \n",
    "        logits_amodal_mask, logits_amodal_content = self.encode_decode(model_input)\n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if self.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        mask_loss, content_loss = self.loss_function(logits_amodal_mask, \n",
    "                                                        amodal_mask_labels,\n",
    "                                                        logits_amodal_content, \n",
    "                                                        amodal_content_labels)\n",
    "        \n",
    "        loss = mask_loss + content_loss\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "        \n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "            'loss': loss.data.item(),\n",
    "            'mask_loss': mask_loss.data.item(),\n",
    "            'content_loss': content_loss.data.item(),\n",
    "            'other_metrics_to_monitor': 0 # add more metrics here - just make sure they are a number\n",
    "        }\n",
    "        return loss, metrics\n",
    "    def loss_function(self,\n",
    "                    amodal_mask_preds,\n",
    "                    amodal_mask_labels,\n",
    "                    amodal_content_preds,\n",
    "                    amodal_content_labels):\n",
    "        mask_loss = self.bce_loss(amodal_mask_preds, amodal_mask_labels)\n",
    "        content_loss = self.mse_loss(amodal_content_preds, amodal_content_labels)\n",
    "        return mask_loss, content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bc17a-1c3d-4440-8393-10c02a376eb5",
   "metadata": {},
   "source": [
    "#### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f028d-4634-4a14-bd15-80082152972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a U-net which expects a batch of 4-channel images as input (batch_size, channels=4, height, width)\n",
    "model = Unet_Image(4)\n",
    "\n",
    "# Make a dummy tensor to test the model can successfully apply its layers and get an output\n",
    "batch_size = 1\n",
    "rand_input = torch.randn((batch_size, 4, 256, 256))\n",
    "\n",
    "# Apply the model to the input - we use encode decode here rather than forward\n",
    "# because we don't have the full batch yet - we will later\n",
    "logits_amodal_mask, logits_amodal_content = model.encode_decode(rand_input)\n",
    "print('Model output:', logits_amodal_mask.shape, logits_amodal_content.shape)\n",
    "\n",
    "# For our use-case, we are predicting amodal masks and amodal content\n",
    "# The amodal mask prediction should be (batch_size, 1, h, w) for a binary mask and the \n",
    "# Amodal content prediction should be  (batch_size, 3, h, w) for an rgb object prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51e1c2-b9b0-4382-b540-6f27ba2f035c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbd7f1-b899-456f-b6e4-7083a61a297d",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678d9a7-aa16-4c5e-a986-0773fd25750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we have a model, a forward call, and a calculated loss to backpropegate and propegate\n",
    "\"\"\"\n",
    "\n",
    "def batch_to_device(batch, device):\n",
    "    for key, value in batch.items():\n",
    "        batch[key] = value.to(device)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def aggregate_metrics(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries containing metrics, aggregate into one dictionary\n",
    "    \"\"\"\n",
    "    mean_dict = {\n",
    "        key: sum(d[key] for d in list_of_dicts) / len(list_of_dicts)\n",
    "        for key in list_of_dicts[0].keys()\n",
    "    }\n",
    "    return mean_dict\n",
    "\n",
    "def val_step(batch, model):\n",
    "    \"\"\"\n",
    "    Take a validation step to get predictions and metrics on a batch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss, metrics = model.forward(batch)\n",
    "    model.train()\n",
    "    return loss, metrics\n",
    "\n",
    "def train_step(batch, model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, metrics = model.forward(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, device):\n",
    "    \"\"\"\n",
    "    Iterate over the \n",
    "    \"\"\"\n",
    "    epoch_metrics = []\n",
    "    for batch in dataloader:\n",
    "        batch = batch_to_device(batch, device)\n",
    "        _, metrics = train_step(batch, model, optimizer)\n",
    "        epoch_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate list of metrics \n",
    "    aggregated_metrics = aggregate_metrics(epoch_metrics)\n",
    "    return aggregated_metrics\n",
    "\n",
    "def valid_epoch(model, dataloader, device):\n",
    "    epoch_metrics = []\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = batch_to_device(batch, device)\n",
    "            _, metrics = val_step(batch, model)\n",
    "            epoch_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate list of metrics \n",
    "    aggregated_metrics = aggregate_metrics(epoch_metrics)\n",
    "    return aggregated_metrics\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, n_epochs, device):\n",
    "    list_of_train_metrics = []\n",
    "    list_of_valid_metrics = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Starting Epoch', epoch)\n",
    "        train_epoch_metrics = train_epoch(model, optimizer, train_dataloader, device)\n",
    "        list_of_train_metrics.append(train_epoch_metrics)\n",
    "\n",
    "        valid_epoch_metrics = valid_epoch(model, val_dataloader, device)\n",
    "        list_of_valid_metrics.append(valid_epoch_metrics)\n",
    "\n",
    "        if epoch%1 == 0:\n",
    "            print(f'Epoch {epoch} metrics:')\n",
    "            format_metrics(train_epoch_metrics, valid_epoch_metrics, epoch)\n",
    "\n",
    "    return list_of_train_metrics, list_of_valid_metrics\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def format_metrics(training_metrics, validation_metrics, epoch):\n",
    "    # Combine the metrics into rows for the table\n",
    "    rows = []\n",
    "    for metric in training_metrics.keys():\n",
    "        train_value = training_metrics.get(metric, \"N/A\")\n",
    "        val_value = validation_metrics.get(metric, \"N/A\")\n",
    "        rows.append([metric, train_value, val_value])\n",
    "    \n",
    "    # Create the table with headers\n",
    "    table = tabulate(rows, headers=[f\"Metric - Epoch {epoch}\", \"Training\", \"Validation\"], tablefmt=\"grid\")\n",
    "    print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a61622-a6e4-439f-a997-9efcac0acae7",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc1371-1a09-4945-8ee2-2a4b9581a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "learning_rate = 3e-5 # 3e-4?...\n",
    "batch_size = 32\n",
    "mask_content_preds = True\n",
    "n_workers = 32\n",
    "n_epochs = 20\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = MOVi_Dataset(split = 'train', \n",
    "                             n_frames = 1,\n",
    "                             n_samples = 64)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              num_workers = n_workers, \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "val_dataset = MOVi_Dataset(split = 'test', \n",
    "                           n_frames = 1,\n",
    "                           n_samples = 32)\n",
    "val_dataloader = DataLoader(train_dataset, \n",
    "                            num_workers = n_workers, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Our model is a torch model which contains the layers we will train.\n",
    "This model takes the RGB image and the modal mask of an object (an attention cue) \n",
    "And will be trained to return the amodal (unobscured) mask as well as the amodal content.\n",
    "Think of this as training for \"x-ray vision\" - given an attention cue, return what the unoccluded object\n",
    "would look like, given hints from the image or video.\n",
    "\n",
    "Extending this to video will be the next step, so start thinking about how you can combine\n",
    "multiple frames of information to predict what an occluded object looks like.\n",
    "\"\"\"\n",
    "\n",
    "device = 0\n",
    "model = Unet_Image(in_channels = 4).to(device) # ModalMask+RGB -> AmodalMask+AmodalContent\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optim, train_dataloader, val_dataloader, n_epochs = n_epochs, device = device)\n",
    "\n",
    "\"\"\"\n",
    "While this trains, you should see the epoch performances greatly improving on the training split.\n",
    "They may or may not improve on the validation split depending on many factors.\n",
    "Now that this code runs though, try turning up the size of the datasets and see what affect that has.\n",
    "\n",
    "Then eventually you can start designing a model intended to track and segment objects from video\n",
    "which will require setting the datasets n_frames to a larger number, and re-designing your torch model.\n",
    "\n",
    "Hopefully everything else remains working though! I would copy this notebook to have around for referce\n",
    "Then make your own notebook where you experiment with new designs! :)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscenv",
   "language": "python",
   "name": "dsc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
