{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29666e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch, Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import write_video\n",
    "\n",
    "# Common\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from IPython.display import Video\n",
    "\n",
    "# Utils from Torchvision\n",
    "tensor_to_image = ToPILImage()\n",
    "image_to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fb1067-52c6-4f0e-ba6b-db15cf74c55b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a02b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(amodal_mask_preds, \n",
    "                      amodal_mask_labels, \n",
    "                      amodal_content_preds,\n",
    "                      amodal_content_labels):\n",
    "    \"\"\"\n",
    "    Here, you can calculate non-loss metrics like mIOU, accuracy, J&F scores.\n",
    "\n",
    "    And non-loss image generation metrics between the predicted and ground-truth amodal content\n",
    "    Such as Inception Score, Frechet Inception Distance, Learned Perceptual Patch Similarity (LPIPS),\n",
    "    Structure Similarity Index Metric (SSIM), Peak Signal-Noise Ratio (PSNR)\n",
    "\n",
    "    These should all have easy-to-use implementations in libraries such as TorchMetrics.\n",
    "    \"\"\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a7455",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37849654",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e6fd40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class conv2d_inplace_spatial(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, pooling_function, activation = nn.GELU()):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation,\n",
    "            pooling_function,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, scale_factor=(2, 2), mode='bilinear', align_corners=False):\n",
    "        super(Upscale, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
    "\n",
    "class Unet_Image(nn.Module):\n",
    "    def __init__(self, in_channels = 4, mask_content_preds = False):\n",
    "        super().__init__()\n",
    "        # downsampling\n",
    "        self.mpool_2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        # down path - encoding - increase dimensions\n",
    "        self.down1 = conv2d_inplace_spatial(in_channels, 32, self.mpool_2)\n",
    "        self.down2 = conv2d_inplace_spatial(32, 64, self.mpool_2)\n",
    "        self.down3 = conv2d_inplace_spatial(64, 128, self.mpool_2)\n",
    "        self.down4 = conv2d_inplace_spatial(128, 256, self.mpool_2)\n",
    "        \n",
    "        # upsample by factor of 2, in both dimensions\n",
    "        self.upscale_2 = Upscale(scale_factor=(2, 2), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # up path - decoder\n",
    "        # slowly reduce dimensions\n",
    "        self.up1 = conv2d_inplace_spatial(256, 128, self.upscale_2)\n",
    "        self.up2 = conv2d_inplace_spatial(256, 64, self.upscale_2)\n",
    "        self.up3 = conv2d_inplace_spatial(128, 32, self.upscale_2)\n",
    "        \n",
    "        # no activation\n",
    "        self.up4_amodal_mask = conv2d_inplace_spatial(64, 1, self.upscale_2, activation = nn.Identity())\n",
    "        self.up4_amodal_content = conv2d_inplace_spatial(64, 3, self.upscale_2, activation = nn.Identity())\n",
    "\n",
    "        # Optional arguments\n",
    "        self.mask_content_preds = mask_content_preds # Should we mask the amodal content prediction by the amodal mask prediction?\n",
    "\n",
    "        # Optimization\n",
    "        self.mse_loss = nn.L1Loss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "        # Self-attention feature enrichment\n",
    "        #max_seq = 16*16*6\n",
    "        #token_dim = 256\n",
    "        #self.pos_enc = nn.Parameter(torch.zeros((max_seq, 1, token_dim))) # seq b dim\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=token_dim, nhead=8)\n",
    "        #self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "    \n",
    "    # Packages the encoder path (downpath)\n",
    "    def encode(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        \n",
    "        # Returns the four\n",
    "        # [torch.Size([6, 32, 4, 128, 128]), torch.Size([6, 64, 2, 64, 64]), torch.Size([6, 128, 1, 32, 32]), torch.Size([6, 256, 1, 16, 16])]\n",
    "        return x1, x2, x3, x4\n",
    "    \n",
    "    # packages the decode path (up-path)\n",
    "    def decode(self, h1, h2, h3, h4):\n",
    "        h4 = self.up1(h4) # 6, 256, 1, 16, 16 -> 6, 128, 1, 32, 32 (double spatial, then conv-in-place channels to half)\n",
    "        h34 = torch.cat((h3, h4), dim = 1) # (6, 2*128, 1, 32, 32)\n",
    "\n",
    "        h34 = self.up2(h34) # 6, 256, 1, 32, 32 -> 6, 128, 2, 64, 64\n",
    "        h234 = torch.cat((h2, h34), dim = 1)\n",
    "\n",
    "        h234 = self.up3(h234)\n",
    "        h1234 = torch.cat((h1, h234), dim = 1)\n",
    "        \n",
    "        logits_amodal_mask = self.up4_amodal_mask(h1234)\n",
    "        logits_amodal_content = self.up4_amodal_content(h1234)\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "    \n",
    "    # Encode - decode together\n",
    "    # Runs through the whole model\n",
    "    def encode_decode(self, x):\n",
    "        \"\"\"\n",
    "        input image tensor: (bs, c, h, w) - batch size, channels, height, width\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # Multiscale features x1, x2, x3, x4\n",
    "        x1, x2, x3, x4 = self.encode(x)\n",
    "\n",
    "        # You could add code here for example more layers that modify the latent x4? Be creative :)\n",
    "\n",
    "        # Decode using enriched features\n",
    "        logits_amodal_mask, logits_amodal_content = self.decode(x1, x2, x3, x4)\n",
    "\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # concat rgb and modal masks as input to model\n",
    "        model_input = torch.cat((batch['frames'], batch['modal_masks']), dim = 1)\n",
    "\n",
    "        # Remove time dimension from a few things (add it back in later when you make your video model!)\n",
    "        model_input = model_input.squeeze(2) # remove time dimension (you will probably want it later!)\n",
    "        amodal_mask_labels = batch['amodal_masks'].float().squeeze(2)\n",
    "        amodal_content_labels = batch['amodal_content'].float().squeeze(2)\n",
    "\n",
    "        # Model input\n",
    "        #print('model input:', model_input.shape)\n",
    "        \n",
    "        logits_amodal_mask, logits_amodal_content = self.encode_decode(model_input)\n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if self.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        mask_loss, content_loss = self.loss_function(logits_amodal_mask, \n",
    "                                                        amodal_mask_labels,\n",
    "                                                        logits_amodal_content, \n",
    "                                                        amodal_content_labels)\n",
    "        \n",
    "        loss = mask_loss + content_loss\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "        \n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "            'loss': loss.data.item(),\n",
    "            'mask_loss': mask_loss.data.item(),\n",
    "            'content_loss': content_loss.data.item(),\n",
    "            'other_metrics_to_monitor': 0 # add more metrics here - just make sure they are a number\n",
    "        }\n",
    "        return loss, metrics\n",
    "    def loss_function(self,\n",
    "                    amodal_mask_preds,\n",
    "                    amodal_mask_labels,\n",
    "                    amodal_content_preds,\n",
    "                    amodal_content_labels):\n",
    "        mask_loss = self.bce_loss(amodal_mask_preds, amodal_mask_labels) # Binary Cross Entropy Loss\n",
    "        content_loss = self.mse_loss(amodal_content_preds, amodal_content_labels) # L1 loss\n",
    "        return mask_loss, content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf3516",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "\n",
    "Test model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e68716",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: torch.Size([1, 1, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Make a U-net which expects a batch of 4-channel images as input (batch_size, channels=4, height, width)\n",
    "model = Unet_Image(4)\n",
    "\n",
    "# Make a dummy tensor to test the model can successfully apply its layers and get an output\n",
    "batch_size = 1\n",
    "rand_input = torch.randn((batch_size, 4, 256, 256))\n",
    "\n",
    "# Apply the model to the input - we use encode decode here rather than forward\n",
    "# because we don't have the full batch yet - we will later\n",
    "logits_amodal_mask, logits_amodal_content = model.encode_decode(rand_input)\n",
    "print('Model output:', logits_amodal_mask.shape, logits_amodal_content.shape)\n",
    "\n",
    "# For our use-case, we are predicting amodal masks and amodal content\n",
    "# The amodal mask prediction should be (batch_size, 1, h, w) for a binary mask and the \n",
    "# Amodal content prediction should be  (batch_size, 3, h, w) for an rgb object prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430e326",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d53f9",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2c6fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # flash has gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1c7cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we have a model, a forward call, and a calculated loss to backpropegate and propegate\n",
    "\"\"\"\n",
    "\n",
    "# Send batch to device\n",
    "def batch_to_device(batch, device):\n",
    "    for key, value in batch.items():\n",
    "        batch[key] = value.to(device)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def aggregate_metrics(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries containing metrics, aggregate into one dictionary\n",
    "    \"\"\"\n",
    "    mean_dict = {\n",
    "        key: sum(d[key] for d in list_of_dicts) / len(list_of_dicts)\n",
    "        for key in list_of_dicts[0].keys()\n",
    "    }\n",
    "    return mean_dict\n",
    "\n",
    "def val_step(batch, model):\n",
    "    \"\"\"\n",
    "    Take a validation step to get predictions and metrics on a batch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss, metrics = model.forward(batch)\n",
    "    model.train()\n",
    "    return loss, metrics\n",
    "\n",
    "def train_step(batch, model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, metrics = model.forward(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, device):\n",
    "    \"\"\"\n",
    "    Iterate over the \n",
    "    \"\"\"\n",
    "    epoch_metrics = []\n",
    "    for batch in dataloader:\n",
    "        batch = batch_to_device(batch, device)\n",
    "        _, metrics = train_step(batch, model, optimizer)\n",
    "        epoch_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate list of metrics \n",
    "    aggregated_metrics = aggregate_metrics(epoch_metrics)\n",
    "    return aggregated_metrics\n",
    "\n",
    "def valid_epoch(model, dataloader, device):\n",
    "    epoch_metrics = []\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = batch_to_device(batch, device)\n",
    "            _, metrics = val_step(batch, model)\n",
    "            epoch_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate list of metrics \n",
    "    aggregated_metrics = aggregate_metrics(epoch_metrics)\n",
    "    return aggregated_metrics\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, n_epochs, device):\n",
    "    list_of_train_metrics = []\n",
    "    list_of_valid_metrics = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Starting Epoch', epoch)\n",
    "        train_epoch_metrics = train_epoch(model, optimizer, train_dataloader, device)\n",
    "        list_of_train_metrics.append(train_epoch_metrics)\n",
    "\n",
    "        valid_epoch_metrics = valid_epoch(model, val_dataloader, device)\n",
    "        list_of_valid_metrics.append(valid_epoch_metrics)\n",
    "\n",
    "        if epoch%1 == 0:\n",
    "            print(f'Epoch {epoch} metrics:')\n",
    "            format_metrics(train_epoch_metrics, valid_epoch_metrics, epoch)\n",
    "\n",
    "    return list_of_train_metrics, list_of_valid_metrics\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def format_metrics(training_metrics, validation_metrics, epoch):\n",
    "    # Combine the metrics into rows for the table\n",
    "    rows = []\n",
    "    for metric in training_metrics.keys():\n",
    "        train_value = training_metrics.get(metric, \"N/A\")\n",
    "        val_value = validation_metrics.get(metric, \"N/A\")\n",
    "        rows.append([metric, train_value, val_value])\n",
    "    \n",
    "    # Create the table with headers\n",
    "    table = tabulate(rows, headers=[f\"Metric - Epoch {epoch}\", \"Training\", \"Validation\"], tablefmt=\"grid\")\n",
    "    print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41201ce0",
   "metadata": {},
   "source": [
    "#### Demo Train\n",
    "\n",
    "Using the provided architecture and calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f366403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from load_data.load_dataset import MOVi_Dataset, MOVi_ImageDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e72df40-2f3e-4047-995f-41191a2f3c75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "except:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e39edd9-fece-49d0-b485-a2100261bce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/p/lustre2/marcou1/dsc2025/dsc25_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5df399",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset init on train\n",
      "Init data top dir: /p/lustre2/marcou1/dsc2025/dsc25_data/train/\n",
      "Dataset init on test\n",
      "Init data top dir: /p/lustre2/marcou1/dsc2025/dsc25_data/test/\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "learning_rate = 3e-4 # 3e-4?...\n",
    "batch_size = 8\n",
    "mask_content_preds = True\n",
    "n_workers = 8\n",
    "n_epochs = 15\n",
    "\n",
    "# Dataloaders\n",
    "# These are specifically for images\n",
    "# load 1 consecutive frame at a time\n",
    "train_dataset = MOVi_Dataset(split = 'train', \n",
    "                             n_frames = 1,\n",
    "                             n_samples = 16, \n",
    "                             root=ROOT_PATH)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              num_workers = n_workers, \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "val_dataset = MOVi_Dataset(split = 'test', \n",
    "                           n_frames = 1,\n",
    "                           n_samples = 8,\n",
    "                          root=ROOT_PATH)\n",
    "val_dataloader = DataLoader(train_dataset, \n",
    "                            num_workers = n_workers, \n",
    "                            batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be44291-f445-4704-93b4-7e08d3733f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5467e59-e4b5-44a0-a9b6-ef4fe35e88bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/p/lustre2/marcou1/dsc2025/groupj_llnl/notebooks/task2/../../load_data/load_dataset.py\", line 129, in __getitem__\n    all_object_ids = self.all_objects(self.top_dir + random_scene + '/camera_0000/' )\n  File \"/p/lustre2/marcou1/dsc2025/groupj_llnl/notebooks/task2/../../load_data/load_dataset.py\", line 193, in all_objects\n    for fname in sorted(os.listdir(pth)):\nFileNotFoundError: [Errno 2] No such file or directory: '/p/lustre2/marcou1/dsc2025/dsc25_data/train/18abb5f48b7d41b580f1f2c52222392d/camera_0000/'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Call the train function\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m list_of_train_metrics, list_of_valid_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mWhile this trains, you should see the epoch performances greatly improving on the training split.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03mThey may or may not improve on the validation split depending on many factors.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mThen make your own notebook where you experiment with new designs! :)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, n_epochs, device)\u001b[0m\n\u001b[1;32m     71\u001b[0m train_epoch_metrics \u001b[38;5;241m=\u001b[39m train_epoch(model, optimizer, train_dataloader, device)\n\u001b[1;32m     72\u001b[0m list_of_train_metrics\u001b[38;5;241m.\u001b[39mappend(train_epoch_metrics)\n\u001b[0;32m---> 74\u001b[0m valid_epoch_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m list_of_valid_metrics\u001b[38;5;241m.\u001b[39mappend(valid_epoch_metrics)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m, in \u001b[0;36mvalid_epoch\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalid_epoch\u001b[39m(model, dataloader, device):\n\u001b[1;32m     55\u001b[0m     epoch_metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     58\u001b[0m             batch \u001b[38;5;241m=\u001b[39m batch_to_device(batch, device)\n",
      "File \u001b[0;32m/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/p/lustre2/marcou1/dsc2025/dsc_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/p/lustre2/marcou1/dsc2025/groupj_llnl/notebooks/task2/../../load_data/load_dataset.py\", line 129, in __getitem__\n    all_object_ids = self.all_objects(self.top_dir + random_scene + '/camera_0000/' )\n  File \"/p/lustre2/marcou1/dsc2025/groupj_llnl/notebooks/task2/../../load_data/load_dataset.py\", line 193, in all_objects\n    for fname in sorted(os.listdir(pth)):\nFileNotFoundError: [Errno 2] No such file or directory: '/p/lustre2/marcou1/dsc2025/dsc25_data/train/18abb5f48b7d41b580f1f2c52222392d/camera_0000/'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Our model is a torch model which contains the layers we will train.\n",
    "This model takes the RGB image and the modal mask of an object (an attention cue) \n",
    "And will be trained to return the amodal (unobscured) mask as well as the amodal content.\n",
    "Think of this as training for \"x-ray vision\" - given an attention cue, return what the unoccluded object\n",
    "would look like, given hints from the image or video.\n",
    "\n",
    "Extending this to video will be the next step, so start thinking about how you can combine\n",
    "multiple frames of information to predict what an occluded object looks like.\n",
    "\"\"\"\n",
    "# Send to cuda\n",
    "model = Unet_Image(in_channels = 4).to(device) # ModalMask+RGB -> AmodalMask+AmodalContent\n",
    "\n",
    "# Optimizer - Adam\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Call the train function\n",
    "list_of_train_metrics, list_of_valid_metrics = train(model, optim, train_dataloader, val_dataloader, n_epochs = n_epochs, device = device)\n",
    "\n",
    "\"\"\"\n",
    "While this trains, you should see the epoch performances greatly improving on the training split.\n",
    "They may or may not improve on the validation split depending on many factors.\n",
    "Now that this code runs though, try turning up the size of the datasets and see what affect that has.\n",
    "\n",
    "Then eventually you can start designing a model intended to track and segment objects from video\n",
    "which will require setting the datasets n_frames to a larger number, and re-designing your torch model.\n",
    "\n",
    "Hopefully everything else remains working though! I would copy this notebook to have around for referce\n",
    "Then make your own notebook where you experiment with new designs! :)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58b2ca-58f4-44f8-be5d-85940b716b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscenv",
   "language": "python",
   "name": "dsc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
