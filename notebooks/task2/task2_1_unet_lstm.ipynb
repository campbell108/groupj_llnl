{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0afcc41",
   "metadata": {},
   "source": [
    "# Task 2.1\n",
    "\n",
    "“Given a video of the modal mask of an object, predict the amodal mask of the same object”\n",
    "\n",
    "Inputs:\n",
    "- RGB Frames - N 3-channel images (N, 3, 256, 256)\n",
    "- N Modal Masks of Object i - N Binary (1-channel) images (N, 1, 256, 256)\n",
    "\n",
    "Outputs:\n",
    "- N Amodal Masks of Object i - N Binary (1-channel) images (1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cfba00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch, Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import write_video\n",
    "\n",
    "# Common\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from IPython.display import Video\n",
    "\n",
    "# Utils from Torchvision\n",
    "tensor_to_image = ToPILImage()\n",
    "image_to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e55d15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee51d768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 16 10:40:53 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla M60                      On  |   00000000:04:00.0 Off |                    0 |\n",
      "| N/A   25C    P8             14W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla M60                      On  |   00000000:05:00.0 Off |                    0 |\n",
      "| N/A   24C    P8             14W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla M60                      On  |   00000000:83:00.0 Off |                    0 |\n",
      "| N/A   24C    P8             13W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla M60                      On  |   00000000:84:00.0 Off |                    0 |\n",
      "| N/A   25C    P8             15W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682926b",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795482dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(amodal_mask_preds, \n",
    "                      amodal_mask_labels, \n",
    "                      amodal_content_preds,\n",
    "                      amodal_content_labels):\n",
    "    \"\"\"\n",
    "    Here, you can calculate non-loss metrics like mIOU, accuracy, J&F scores.\n",
    "\n",
    "    And non-loss image generation metrics between the predicted and ground-truth amodal content\n",
    "    Such as Inception Score, Frechet Inception Distance, Learned Perceptual Patch Similarity (LPIPS),\n",
    "    Structure Similarity Index Metric (SSIM), Peak Signal-Noise Ratio (PSNR)\n",
    "\n",
    "    These should all have easy-to-use implementations in libraries such as TorchMetrics.\n",
    "    \"\"\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be1833c-f61f-4bf8-8cf7-f939285a8de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dice and iOU\n",
    "def dice_loss(preds, targets, smooth=1.):\n",
    "    \"\"\"\n",
    "    Dice coefficient measures overlap between predicted and ground truth masks.\n",
    "    \"\"\"\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    intersection = (preds * targets).sum()\n",
    "    dice = (2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "def iou_loss(preds, targets, smooth=1.):\n",
    "    \"\"\"\n",
    "    IoU (Intersection over Union) - \n",
    "    Intersection of Predicted and True masks\n",
    "    over Union of intersection and true\n",
    "    \"\"\"\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    intersection = (preds * targets).sum()\n",
    "    total = preds.sum() + targets.sum()\n",
    "    union = total - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return 1 - iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2122d40-bd2b-4f41-acbd-9d835bad07e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accuracy(logits_amodal_mask, amodal_mask_labels):\n",
    "    # Compute accuracy\n",
    "    # If preds are probabilities/logits, binarize them:\n",
    "    preds_mask = logits_amodal_mask.sigmoid().round() # make binary\n",
    "    true_mask = amodal_mask_labels.round() # make these binary as well\n",
    "    # Ensure labels are also float or int for comparison\n",
    "    # Calculate number of correct pixels\n",
    "    correct = (preds_mask == true_mask).float().sum()\n",
    "    # Calculate total number of pixels (numel = number of total elements)\n",
    "    total = torch.numel(preds_mask)\n",
    "    # Accuracy\n",
    "    accuracy = correct / total # this is a number 0 to 1\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176772e2",
   "metadata": {},
   "source": [
    "#### Class - Unet_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9771b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class conv2d_inplace_spatial(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, pooling_function, activation = nn.GELU()):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation,\n",
    "            pooling_function,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, scale_factor=(2, 2), mode='bilinear', align_corners=False):\n",
    "        super(Upscale, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
    "\n",
    "class Unet_Image(nn.Module):\n",
    "    def __init__(self, in_channels = 4, mask_content_preds = False):\n",
    "        super().__init__()\n",
    "        # downsampling\n",
    "        # MaxPooling - keep max value over a 2x2 sliding window\n",
    "        self.mpool_2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        # down path - encoding - increase dimensions\n",
    "        self.down1 = conv2d_inplace_spatial(in_channels, 32, self.mpool_2) # 32 x 32\n",
    "        self.down2 = conv2d_inplace_spatial(32, 64, self.mpool_2) # 64 x64 \n",
    "        self.down3 = conv2d_inplace_spatial(64, 128, self.mpool_2) # 128 x128\n",
    "        self.down4 = conv2d_inplace_spatial(128, 256, self.mpool_2) # outputs 256 x256\n",
    "        \n",
    "        # upsample by factor of 2, in both dimensions\n",
    "        self.upscale_2 = Upscale(scale_factor=(2, 2), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # up path - decoder\n",
    "        # slowly reduce dimensions\n",
    "        self.up1 = conv2d_inplace_spatial(256, 128, self.upscale_2)\n",
    "        self.up2 = conv2d_inplace_spatial(256, 64, self.upscale_2)\n",
    "        self.up3 = conv2d_inplace_spatial(128, 32, self.upscale_2)\n",
    "        \n",
    "        # no activation\n",
    "        self.up4_amodal_mask = conv2d_inplace_spatial(64, 1, self.upscale_2, activation = nn.Identity())\n",
    "        self.up4_amodal_content = conv2d_inplace_spatial(64, 3, self.upscale_2, activation = nn.Identity())\n",
    "\n",
    "        # Optional arguments\n",
    "        self.mask_content_preds = mask_content_preds # Should we mask the amodal content prediction by the amodal mask prediction?\n",
    "\n",
    "        # Optimization\n",
    "        self.mse_loss = nn.L1Loss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "        # Self-attention feature enrichment\n",
    "        #max_seq = 16*16*6\n",
    "        #token_dim = 256\n",
    "        #self.pos_enc = nn.Parameter(torch.zeros((max_seq, 1, token_dim))) # seq b dim\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=token_dim, nhead=8)\n",
    "        #self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "    \n",
    "    # Packages the encoder path (downpath)\n",
    "    def encode(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        \n",
    "        # Returns the four\n",
    "        # [torch.Size([6, 32, 4, 128, 128]), torch.Size([6, 64, 2, 64, 64]), torch.Size([6, 128, 1, 32, 32]), torch.Size([6, 256, 1, 16, 16])]\n",
    "        return x1, x2, x3, x4\n",
    "    \n",
    "    # packages the decode path (up-path)\n",
    "    def decode(self, h1, h2, h3, h4):\n",
    "        h4 = self.up1(h4) # 6, 256, 1, 16, 16 -> 6, 128, 1, 32, 32 (double spatial, then conv-in-place channels to half)\n",
    "        h34 = torch.cat((h3, h4), dim = 1) # (6, 2*128, 1, 32, 32)\n",
    "\n",
    "        h34 = self.up2(h34) # 6, 256, 1, 32, 32 -> 6, 128, 2, 64, 64\n",
    "        h234 = torch.cat((h2, h34), dim = 1)\n",
    "\n",
    "        h234 = self.up3(h234)\n",
    "        h1234 = torch.cat((h1, h234), dim = 1)\n",
    "        \n",
    "        logits_amodal_mask = self.up4_amodal_mask(h1234)\n",
    "        logits_amodal_content = self.up4_amodal_content(h1234)\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "    \n",
    "    # Encode - decode together\n",
    "    # Runs through the whole model\n",
    "    def encode_decode(self, x):\n",
    "        \"\"\"\n",
    "        input image tensor: (bs, c, h, w) - batch size, channels, height, width\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # Multiscale features x1, x2, x3, x4\n",
    "        x1, x2, x3, x4 = self.encode(x)\n",
    "\n",
    "        # You could add code here for example more layers that modify the latent x4? Be creative :)\n",
    "\n",
    "        # Decode using enriched features\n",
    "        logits_amodal_mask, logits_amodal_content = self.decode(x1, x2, x3, x4)\n",
    "\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # This takes in a batch\n",
    "        # RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "        # modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "        # concat rgb and modal masks as input to model\n",
    "        model_input = torch.cat((batch['frames'], batch['modal_masks']), dim = 1)\n",
    "\n",
    "        # Remove time dimension from a few things (add it back in later when you make your video model!)\n",
    "        model_input = model_input.squeeze(2) # remove time dimension (you will probably want it later!)\n",
    "        # This is what we use for loss calculations\n",
    "        # These both have range 0 to 1 - same to logits\n",
    "        amodal_mask_labels = batch['amodal_masks'].float().squeeze(2)\n",
    "        amodal_content_labels = batch['amodal_content'].float().squeeze(2)\n",
    "\n",
    "        # Model input\n",
    "        #print('model input:', model_input.shape)\n",
    "        \n",
    "        logits_amodal_mask, logits_amodal_content = self.encode_decode(model_input)\n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if self.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        mask_loss, content_loss = self.loss_function(logits_amodal_mask, \n",
    "                                                        amodal_mask_labels,\n",
    "                                                        logits_amodal_content, \n",
    "                                                        amodal_content_labels)\n",
    "        \n",
    "        loss = mask_loss + content_loss\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "        \n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "            'loss': loss.data.item(),\n",
    "            'mask_loss': mask_loss.data.item(),\n",
    "            'content_loss': content_loss.data.item(),\n",
    "            'other_metrics_to_monitor': 0 # add more metrics here - just make sure they are a number\n",
    "        }\n",
    "        return loss, metrics\n",
    "    def loss_function(self,\n",
    "                    amodal_mask_preds,\n",
    "                    amodal_mask_labels,\n",
    "                    amodal_content_preds,\n",
    "                    amodal_content_labels):\n",
    "        # Compares raw logits for binary prediction - object vs no object\n",
    "        mask_loss = self.bce_loss(amodal_mask_preds, amodal_mask_labels) # Binary Cross Entropy Loss\n",
    "        # Pixel by pixel difference in the RGB\n",
    "        content_loss = self.mse_loss(amodal_content_preds, amodal_content_labels) # L1 loss\n",
    "        return mask_loss, content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a285f",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "\n",
    "Test model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a540b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a U-net which expects a batch of 4-channel images as input (batch_size, channels=4, height, width)\n",
    "model = Unet_Image(4)\n",
    "\n",
    "# Make a dummy tensor to test the model can successfully apply its layers and get an output\n",
    "batch_size = 1\n",
    "rand_input = torch.randn((batch_size, 4, 256, 256))\n",
    "\n",
    "# Apply the model to the input - we use encode decode here rather than forward\n",
    "# because we don't have the full batch yet - we will later\n",
    "logits_amodal_mask, logits_amodal_content = model.encode_decode(rand_input)\n",
    "print('Model output:', logits_amodal_mask.shape, logits_amodal_content.shape)\n",
    "del logits_amodal_mask, logits_amodal_content\n",
    "# For our use-case, we are predicting amodal masks and amodal content\n",
    "# The amodal mask prediction should be (batch_size, 1, h, w) for a binary mask and the \n",
    "# Amodal content prediction should be  (batch_size, 3, h, w) for an rgb object prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d491c75",
   "metadata": {},
   "source": [
    "## ConvLSTM + Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184cd18a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# This processes one time step at a time\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        # 2d convolution\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "# Stacks multiple LSTM cells on top of each other\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W (T=time)\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297f0aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Unet_Image_ConvLSTM(Unet_Image):\n",
    "    def __init__(self, in_channels=4, mask_content_preds=False,\n",
    "                mask_weight=1.0, dice_weight=1.0, iou_weight=0.0, content_weight=1.0):\n",
    "        super().__init__(in_channels, mask_content_preds)\n",
    "        # Inserting the convLSTM from above\n",
    "        # One layer\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_dim=256,        # matches self.down4 out_channels\n",
    "            hidden_dim=256,       # can be same as input_dim or different\n",
    "            kernel_size=(3, 3),\n",
    "            num_layers=1,\n",
    "            batch_first=True,     # input should be (batch, time, channels, h, w)\n",
    "            bias=True,\n",
    "            return_all_layers=False\n",
    "        )\n",
    "        \n",
    "        # Define loss function details\n",
    "        self.mse_loss = nn.L1Loss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        # Loss weights\n",
    "        self.mask_weight = mask_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.iou_weight = iou_weight\n",
    "        self.content_weight = content_weight\n",
    "\n",
    "\n",
    "    def encode_frames(self, x, n_frames):\n",
    "        # Encode each frame\n",
    "        # Remember time dimension is in the second \n",
    "        x1_seq, x2_seq, x3_seq, x4_seq = [], [], [], []\n",
    "        for t in range(n_frames):\n",
    "            # Here we pass all batches (dim=0), for this timestep (dim=1)\n",
    "            x1, x2, x3, x4 = self.encode(x[:, t, :, :])  # Each: (bs, channels, h, w)\n",
    "            x1_seq.append(x1)\n",
    "            x2_seq.append(x2)\n",
    "            x3_seq.append(x3)\n",
    "            x4_seq.append(x4)\n",
    "        # Stack along time dimension\n",
    "        x1_seq = torch.stack(x1_seq, dim=1)  # (bs, n_frames, channels, h, w)\n",
    "        x2_seq = torch.stack(x2_seq, dim=1)\n",
    "        x3_seq = torch.stack(x3_seq, dim=1)\n",
    "        x4_seq = torch.stack(x4_seq, dim=1)\n",
    "        return x1_seq, x2_seq, x3_seq, x4_seq\n",
    "    \n",
    "    \n",
    "    def decode_frames(self, x1_seq, x2_seq, x3_seq, convlstm_out, n_frames):\n",
    "        logits_amodal_mask_seq = []\n",
    "        logits_amodal_content_seq = []\n",
    "\n",
    "        # Time dimension is in dim = 1 (second dimension)\n",
    "        for t in range(n_frames):\n",
    "            h4 = convlstm_out[:, t]  # (bs, channels, h, w)\n",
    "            # Use skip connections from the corresponding frame\n",
    "            h1 = x1_seq[:, t]\n",
    "            h2 = x2_seq[:, t]\n",
    "            h3 = x3_seq[:, t]\n",
    "            logits_mask, logits_content = self.decode(h1, h2, h3, h4)\n",
    "            logits_amodal_mask_seq.append(logits_mask)\n",
    "            logits_amodal_content_seq.append(logits_content)\n",
    "\n",
    "        # Stack to get (bs, n_frames, ...)\n",
    "        logits_amodal_mask_seq = torch.stack(logits_amodal_mask_seq, dim=1)\n",
    "        logits_amodal_content_seq = torch.stack(logits_amodal_content_seq, dim=1)\n",
    "        return logits_amodal_mask_seq, logits_amodal_content_seq\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # This takes in a batch\n",
    "        # RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "        # modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "        # concat rgb and modal masks as input to model\n",
    "\n",
    "        # frames: (bs, c, n_frames, h, w)\n",
    "        # modal_masks: (bs, 1, n_frames, h, w)\n",
    "        frames = batch['modal_rgb']         # (bs, c, n_frames, h, w)\n",
    "        modal_masks = batch['modal_masks']  # (bs, 1, n_frames, h, w)\n",
    "        # mask the frames - this is masked modal RGB\n",
    "        frames = modal_masks * frames \n",
    "        # bs, c, n, h, w = frames.shape\n",
    "        \n",
    "\n",
    "        # # Concatenate along channel dim: (bs, c+1, n_frames, h, w)\n",
    "        # model_input = torch.cat((frames, modal_masks), dim=1)\n",
    "\n",
    "        # This time we don't squeeze the time dimension out\n",
    "        # Prep for loss\n",
    "        # This is what we use for loss calculations\n",
    "        # Remember these are bs, c, n, h, w!\n",
    "        # These both have range 0 to 1 - same to logits\n",
    "        amodal_mask_labels = batch['amodal_masks'].float()\n",
    "        amodal_content_labels = batch['amodal_content'].float()\n",
    "\n",
    "        # Implementation here\n",
    "        # # Concatenate along channel dim: (bs, c+1, n_frames, h, w)\n",
    "        # model_input = torch.cat((frames, modal_masks), dim=1)\n",
    "        x = torch.cat([frames, modal_masks], dim=1)  # (bs, 4, n_frames, h, w)\n",
    "        bs, c, n_frames, h, w = x.shape\n",
    "        # Unet_Image encode expects  (bs, 4, n_frames, h, w)\n",
    "        # Permutation takes (bs, c, n_frames, h, w) --> (bs, n_frames, c, h, w)\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # (bs, n_frames, c, h, w) --> LSTM expects this\n",
    "\n",
    "        # Encode frames\n",
    "        x1_seq, x2_seq, x3_seq, x4_seq = self.encode_frames(x, n_frames)\n",
    "\n",
    "        # Pass bottleneck (x4_seq) through ConvLSTM\n",
    "        # Last layer should match dims from encode: torch.Size([6, 256, 1, 16, 16])]\n",
    "        # ConvLSTM expects B, T, C, H, W\n",
    "        convlstm_out_list, _ = self.convlstm(x4_seq)  # List of outputs for each layer\n",
    "        # Returns layer_output_list, last_state_list\n",
    "        # We only care about the layer output list\n",
    "        convlstm_out = convlstm_out_list[0]           # (bs, n_frames, channels, h, w)\n",
    "\n",
    "        # Decoding frames\n",
    "        # Pass in the sequences of frames and the output of the LSTM\n",
    "        # Will use the last frame for decoding\n",
    "        logits_amodal_mask, logits_amodal_content = self.decode_frames(x1_seq, x2_seq, x3_seq, convlstm_out, n_frames)\n",
    "        \n",
    "        # Reshape logits - use permute\n",
    "        # They come out as bs, n_frames, c, h, w\n",
    "        # example:\n",
    "        # torch.Size([8, 8, 3, 256, 256]) torch.Size([8, 8, 1, 256, 256])\n",
    "        # Batch ground truth is: (bs, c, n_frames, h, w)\n",
    "        # Example\n",
    "        # torch.Size([8, 3, 8, 256, 256]) torch.Size([8, 1, 8, 256, 256])\n",
    "        logits_amodal_mask = logits_amodal_mask.permute(0, 2, 1, 3, 4)\n",
    "        logits_amodal_content = logits_amodal_content.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if self.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        total_loss, mask_loss, d_loss, i_loss, content_loss = self.loss_function(logits_amodal_mask, \n",
    "                                                                                 amodal_mask_labels,\n",
    "                                                                                 logits_amodal_content, \n",
    "                                                                                 amodal_content_labels,\n",
    "                                                                                 mask_weight=self.mask_weight,\n",
    "                                                                                 dice_weight=self.dice_weight,\n",
    "                                                                                 iou_weight=self.iou_weight,\n",
    "                                                                                 content_weight=self.content_weight)\n",
    "\n",
    "        loss = total_loss\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = get_accuracy(logits_amodal_mask, amodal_mask_labels) # this is a number 0 to 1\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "        \n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "        'loss': loss.data.item(),\n",
    "        'mask_loss': mask_loss.data.item(),\n",
    "        'dice_loss': d_loss.data.item(),\n",
    "        'iou_loss': i_loss.data.item(),\n",
    "        'content_loss': content_loss.data.item(),\n",
    "        'accuracy': accuracy.item()\n",
    "        }\n",
    "        return loss, metrics\n",
    "    \n",
    "    def loss_function(self,\n",
    "                  amodal_mask_preds,\n",
    "                  amodal_mask_labels,\n",
    "                  amodal_content_preds,\n",
    "                  amodal_content_labels,\n",
    "                  mask_weight=1.0,\n",
    "                  dice_weight=1.0,\n",
    "                  iou_weight=0.0,  # Set to 1.0 if you want to use IoU loss\n",
    "                  content_weight=1.0):\n",
    "        # BCE Loss\n",
    "        mask_loss = self.bce_loss(amodal_mask_preds, amodal_mask_labels)\n",
    "        # Dice Loss\n",
    "        d_loss = dice_loss(amodal_mask_preds, amodal_mask_labels)\n",
    "        # IoU Loss (optional)\n",
    "        i_loss = iou_loss(amodal_mask_preds, amodal_mask_labels)\n",
    "        # L1 Loss for content\n",
    "        content_loss = self.mse_loss(amodal_content_preds, amodal_content_labels)\n",
    "        # Combine\n",
    "        total_mask_loss = mask_weight * mask_loss + dice_weight * d_loss + iou_weight * i_loss\n",
    "        total_loss = total_mask_loss + content_weight * content_loss\n",
    "        return total_loss, mask_loss, d_loss, i_loss, content_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03b925",
   "metadata": {},
   "source": [
    "### Model dummy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428f14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a U-net which expects a batch of 4-channel images as input (batch_size, channels=4, height, width)\n",
    "model = Unet_Image_ConvLSTM(in_channels=4, mask_content_preds=True)\n",
    "\n",
    "# Make a dummy tensor to test the model can successfully apply its layers and get an output\n",
    "# RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "# modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "# all in all batch is (bs, 4, n_frames, h, w)\n",
    "batch_size = 8\n",
    "rand_input = torch.randn((batch_size, 4, 8, 256, 256))\n",
    "bs, c, n, h, w = rand_input.shape\n",
    "\n",
    "bs, c, n_frames, h, w = rand_input.shape\n",
    "# Unet_Image encode expects  (bs, 4, n_frames, h, w)\n",
    "rand_input = rand_input.permute(0, 2, 1, 3, 4)  # (bs, n_frames, c, h, w) --> LSTM expects this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For our use-case, we are predicting amodal masks and amodal content\n",
    "# The amodal mask prediction should be (batch_size, 1, h, w) for a binary mask and the \n",
    "# Amodal content prediction should be  (batch_size, 3, h, w) for an rgb object prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178acacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_input.shape\n",
    "# bs, c, n_frames, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be55637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These both have range 0 to 1 - same to logits\n",
    "amodal_mask_labels = rand_input[:, :, 0, :, :].float().unsqueeze(2)\n",
    "amodal_content_labels = rand_input[:, :, 1:, :, :].float()\n",
    "\n",
    "print(amodal_mask_labels.shape, amodal_content_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d0361",
   "metadata": {},
   "source": [
    "#### Test layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798a902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding frame by frame\n",
    "with torch.no_grad():\n",
    "    x1_seq, x2_seq, x3_seq, x4_seq = model.encode_frames(x=rand_input, n_frames=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291c48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x4_seq.shape # bs, n_frames, 256, 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92951510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass bottleneck (x4_seq) through ConvLSTM\n",
    "# Last layer should match dims from encode: torch.Size([6, 256, 1, 16, 16])]\n",
    "# ConvLSTM expects B, T, C, H, W\n",
    "with torch.no_grad():\n",
    "    convlstm_out_list, _ = model.convlstm(x4_seq)  # List of outputs for each layer\n",
    "    # Returns layer_output_list, last_state_list\n",
    "    # We only care about the layer output list\n",
    "    convlstm_out = convlstm_out_list[0]           # (bs, n_frames, channels, h, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b24c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(convlstm_out_list) # 1 layer --> length 1 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33929bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(convlstm_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ce395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decoding frames\n",
    "# Pass in the sequences of frames and the output of the LSTM\n",
    "# Will use the last frame for decoding\n",
    "with torch.no_grad():\n",
    "    logits_amodal_mask, logits_amodal_content = model.decode_frames(x1_seq, x2_seq, x3_seq, convlstm_out, n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fa9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(logits_amodal_content.shape, logits_amodal_mask.shape)\n",
    "# bs, n_frames, c, h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873b11f-bb6f-4c0d-884a-4a910416f796",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bb467-af11-495c-abcd-f7100829d62b",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2c6fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # flash has gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1c7cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we have a model, a forward call, and a calculated loss to backpropegate and propegate\n",
    "\"\"\"\n",
    "\n",
    "# Send batch to device\n",
    "def batch_to_device(batch, device):\n",
    "    for key, value in batch.items():\n",
    "        if key != 'metadata':\n",
    "            batch[key] = value.to(device)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def aggregate_metrics(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries containing metrics, aggregate into one dictionary\n",
    "    \"\"\"\n",
    "    mean_dict = {\n",
    "        key: sum(d[key] for d in list_of_dicts) / len(list_of_dicts)\n",
    "        for key in list_of_dicts[0].keys()\n",
    "    }\n",
    "    return mean_dict\n",
    "\n",
    "def val_step(batch, model):\n",
    "    \"\"\"\n",
    "    Take a validation step to get predictions and metrics on a batch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss, metrics = model.forward(batch)\n",
    "    model.train()\n",
    "    return loss, metrics\n",
    "\n",
    "def train_step(batch, model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss, metrics = model.forward(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, device):\n",
    "    \"\"\"\n",
    "    Iterate over the batches\n",
    "    \"\"\"\n",
    "    epoch_metrics = []\n",
    "    for batch in dataloader:\n",
    "        batch = batch_to_device(batch, device)\n",
    "        _, metrics = train_step(batch, model, optimizer)\n",
    "        epoch_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate list of metrics \n",
    "    aggregated_metrics = aggregate_metrics(epoch_metrics)\n",
    "    return aggregated_metrics\n",
    "\n",
    "def valid_epoch(model, dataloader, device):\n",
    "    epoch_metrics = []\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = batch_to_device(batch, device)\n",
    "            _, metrics = val_step(batch, model)\n",
    "            epoch_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate list of metrics \n",
    "    aggregated_metrics = aggregate_metrics(epoch_metrics)\n",
    "    return aggregated_metrics\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, n_epochs, device, print_cuda_malloc=True):\n",
    "    list_of_train_metrics = []\n",
    "    list_of_valid_metrics = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Starting Epoch', epoch)\n",
    "        if print_cuda_malloc:\n",
    "            print(\"CUDA Malloc:\")\n",
    "            print(f\"{torch.cuda.memory_allocated()/1e6} MB\")\n",
    "        \n",
    "        train_epoch_metrics = train_epoch(model, optimizer, train_dataloader, device)\n",
    "        list_of_train_metrics.append(train_epoch_metrics)\n",
    "\n",
    "        valid_epoch_metrics = valid_epoch(model, val_dataloader, device)\n",
    "        list_of_valid_metrics.append(valid_epoch_metrics)\n",
    "\n",
    "        if epoch%1 == 0:\n",
    "            print(f'Epoch {epoch} metrics:')\n",
    "            format_metrics(train_epoch_metrics, valid_epoch_metrics, epoch)\n",
    "\n",
    "    return list_of_train_metrics, list_of_valid_metrics\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def format_metrics(training_metrics, validation_metrics, epoch):\n",
    "    # Combine the metrics into rows for the table\n",
    "    rows = []\n",
    "    for metric in training_metrics.keys():\n",
    "        train_value = training_metrics.get(metric, \"N/A\")\n",
    "        val_value = validation_metrics.get(metric, \"N/A\")\n",
    "        rows.append([metric, train_value, val_value])\n",
    "    \n",
    "    # Create the table with headers\n",
    "    table = tabulate(rows, headers=[f\"Metric - Epoch {epoch}\", \"Training\", \"Validation\"], tablefmt=\"grid\")\n",
    "    print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e469b63-18ff-4c0d-81b1-650f8a73aefa",
   "metadata": {},
   "source": [
    "#### Demo Train\n",
    "\n",
    "Using the provided architecture and calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29bf8a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f366403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from load_data.load_dataset import MOVi_Dataset, MOVi_ImageDataset, MOVi_Dataset_Filtered\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e72df40-2f3e-4047-995f-41191a2f3c75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "except:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# force cpu\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5060d-630f-4415-942a-da624c4e6c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c225c6f-5159-471f-a1b4-02ff5c6bc1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 16 10:41:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla M60                      On  |   00000000:04:00.0 Off |                    0 |\n",
      "| N/A   25C    P8             15W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla M60                      On  |   00000000:05:00.0 Off |                    0 |\n",
      "| N/A   24C    P8             13W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla M60                      On  |   00000000:83:00.0 Off |                    0 |\n",
      "| N/A   24C    P8             14W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla M60                      On  |   00000000:84:00.0 Off |                    0 |\n",
      "| N/A   25C    P8             14W /  150W |       3MiB /   7680MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e39edd9-fece-49d0-b485-a2100261bce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/Users/marcou1/Library/CloudStorage/OneDrive-LLNL/DSSI/DataSciChallenge/data\"\n",
    "ROOT_PATH = \"/p/lustre2/marcou1/dsc2025/dsc25_data\"\n",
    "ROOT_PATH = \"/usr/workspace/dssi-dsc/subset_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a5df399",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset init on train\n",
      "Init data top dir: /usr/workspace/dssi-dsc/subset_data/train/\n",
      "Dataset init on test\n",
      "Init data top dir: /usr/workspace/dssi-dsc/subset_data/test/\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "learning_rate = 7e-5 # 3e-4?... or 3e-5\n",
    "batch_size = 4\n",
    "mask_content_preds = True\n",
    "n_workers = 1\n",
    "n_epochs = 10\n",
    "n_frames = 8\n",
    "\n",
    "# Dataloaders\n",
    "# These are specifically for images\n",
    "# load 1 consecutive frame at a time\n",
    "train_dataset = MOVi_Dataset_Filtered(split = 'train', \n",
    "                             n_frames = n_frames,\n",
    "                             n_samples = 128, \n",
    "                             root=ROOT_PATH)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              num_workers = n_workers, \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "val_dataset = MOVi_Dataset_Filtered(split = 'test', \n",
    "                           n_frames = n_frames,\n",
    "                           n_samples = 16,\n",
    "                          root=ROOT_PATH)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            num_workers = n_workers, \n",
    "                            batch_size=batch_size)\n",
    "# val_dataset = train_dataset\n",
    "\n",
    "# val_dataloader = train_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8813c-1ec5-436d-aa9e-cc2a1c407346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506092a6-3786-46e6-b6e1-9eac46bd0a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = next(iter(val_dataset))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c18f4-bf41-4416-88ed-0c62749aa677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392035a-9188-4353-9eef-a64e5c57031c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d30600-42a7-44a2-9985-760835465d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Report on foreground ratio\n",
    "fg_ratio = []\n",
    "loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "for batch in tqdm(loader):\n",
    "    fg_ratio.append(batch['metadata']['foreground_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25912e98-27ec-43ee-aae9-6e3ec2f342ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fg_ratio = np.array(fg_ratio).flatten()\n",
    "print(np.nanmedian(fg_ratio), np.nanmean(fg_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739cb512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Take a random sample from your train dataset\n",
    "idx = random.randint(0, len(val_dataset)-1)\n",
    "sample = train_dataset[idx]  # sample is a dict\n",
    "\n",
    "dep = sample['depths'][0][0]\n",
    "print(dep.min(), dep.max())\n",
    "plt.imshow(dep)\n",
    "plt.show()\n",
    "\n",
    "rgb = sample['frames'][:, 0].permute(1,2,0)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"RGB\")\n",
    "plt.show()\n",
    "\n",
    "mmask = sample['modal_masks'][:, 0].permute(1,2,0)\n",
    "print(mmask.min(), mmask.max())\n",
    "plt.imshow(mmask, cmap='gray')\n",
    "plt.title(\"modal mask\")\n",
    "plt.show()\n",
    "\n",
    "amask = sample['amodal_masks'][:, 0].permute(1,2,0)\n",
    "print(amask.min(), amask.max())\n",
    "plt.imshow(amask, cmap='gray')\n",
    "plt.title(\"amodal mask\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(sample['amodal_content'][:, 0].permute(1,2,0))\n",
    "plt.title(\"amodal RGB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b95510-8353-4820-b85e-f4da94ea1688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43c84f-9545-45aa-bd67-d0282f89d80a",
   "metadata": {},
   "source": [
    "### Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5706d93-1daf-4b66-9c64-8d53285c0455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, optim, list_of_train_metrics, list_of_valid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7f56e5-b882-4f80-8159-74e806fed3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0ade7-01be-484c-995b-0283ca4b8091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 7e-5 # 3e-4?... or 3e-5\n",
    "mask_content_preds = True\n",
    "n_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5467e59-e4b5-44a0-a9b6-ef4fe35e88bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Unet_Image_ConvLSTM training for mask prediction...\n",
      "Starting Epoch 0\n",
      "Epoch 0 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 0   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.51184  |     2.54761  |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.634956 |     0.635589 |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.919369 |     0.941968 |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.957515 |     0.970049 |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.13027  |     0.107919 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.966326 |     0.970157 |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 1\n",
      "Epoch 1 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 1   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.44521  |     2.49498  |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.611277 |     0.593213 |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.891999 |     0.935292 |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.941931 |     0.966474 |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.149203 |     0.076884 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.967547 |     0.981548 |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 2\n",
      "Epoch 2 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 2   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.4741   |     2.41558  |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.615336 |     0.608015 |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.907844 |     0.876049 |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.950922 |     0.931514 |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.132208 |     0.154486 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.975033 |     0.977448 |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 3\n",
      "Epoch 3 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 3   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.44543  |     2.47436  |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.608607 |     0.617405 |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.894148 |     0.906404 |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.942674 |     0.950555 |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.146157 |     0.155572 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.964863 |     0.968486 |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 4\n",
      "Epoch 4 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 4   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.45758  |    2.51219   |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.611683 |    0.600717  |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.899829 |    0.941777  |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.94607  |    0.969694  |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.141301 |    0.0967022 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.966251 |    0.958632  |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 5\n",
      "Epoch 5 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 5   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.46581  |    2.54247   |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.611323 |    0.607822  |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.905186 |    0.956759  |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.9493   |    0.977891  |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.133006 |    0.0635932 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.97599  |    0.981863  |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 6\n",
      "Epoch 6 metrics:\n",
      "+--------------------+------------+--------------+\n",
      "| Metric - Epoch 6   |   Training |   Validation |\n",
      "+====================+============+==============+\n",
      "| loss               |   2.43944  |     2.39382  |\n",
      "+--------------------+------------+--------------+\n",
      "| mask_loss          |   0.604691 |     0.580667 |\n",
      "+--------------------+------------+--------------+\n",
      "| dice_loss          |   0.892652 |     0.879798 |\n",
      "+--------------------+------------+--------------+\n",
      "| iou_loss           |   0.9421   |     0.93335  |\n",
      "+--------------------+------------+--------------+\n",
      "| content_loss       |   0.144993 |     0.140578 |\n",
      "+--------------------+------------+--------------+\n",
      "| accuracy           |   0.968875 |     0.969519 |\n",
      "+--------------------+------------+--------------+\n",
      "Starting Epoch 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the train function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m list_of_train_metrics, list_of_valid_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_cuda_malloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 76\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, n_epochs, device, print_cuda_malloc)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA Malloc:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m train_epoch_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m list_of_train_metrics\u001b[38;5;241m.\u001b[39mappend(train_epoch_metrics)\n\u001b[1;32m     79\u001b[0m valid_epoch_metrics \u001b[38;5;241m=\u001b[39m valid_epoch(model, val_dataloader, device)\n",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     47\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch_to_device(batch, device)\n\u001b[0;32m---> 48\u001b[0m     _, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     epoch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Aggregate list of metrics \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(batch, model, optimizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[10], line 162\u001b[0m, in \u001b[0;36mUnet_Image_ConvLSTM.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    155\u001b[0m metrics \u001b[38;5;241m=\u001b[39m calculate_metrics(logits_amodal_mask, \n\u001b[1;32m    156\u001b[0m                             amodal_mask_labels, \n\u001b[1;32m    157\u001b[0m                             logits_amodal_content,\n\u001b[1;32m    158\u001b[0m                             amodal_content_labels)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Report the metrics we calculated in addition to our loss functions\u001b[39;00m\n\u001b[1;32m    161\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: mask_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdice_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: d_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: i_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: content_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    168\u001b[0m }\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, metrics\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Initiating Unet_Image_ConvLSTM training for mask prediction...\")\n",
    "\n",
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Send to cuda\n",
    "model = Unet_Image_ConvLSTM(in_channels = 4, \n",
    "                            mask_content_preds=True,\n",
    "                            mask_weight=1.0,\n",
    "                            dice_weight=1.0,\n",
    "                            iou_weight=1.0, \n",
    "                            content_weight=0.).to(device) # Object-wise ModalMask+RGB -> Object-wise AmodalMask+AmodalContent\n",
    "\n",
    "# Optimizer - Adam\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Call the train function\n",
    "list_of_train_metrics, list_of_valid_metrics = train(model, optim, train_dataloader, val_dataloader, n_epochs = n_epochs, device = device, print_cuda_malloc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58b2ca-58f4-44f8-be5d-85940b716b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = {'train': {}, 'validation': {}}\n",
    "for key in list_of_train_metrics[0].keys():\n",
    "    metrics['train'][key] = [list_of_train_metrics[i][key] for i in range(len(list_of_train_metrics))]\n",
    "\n",
    "for key in list_of_valid_metrics[0].keys():\n",
    "    metrics['validation'][key] = [list_of_valid_metrics[i][key] for i in range(len(list_of_valid_metrics))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc12e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "for key in ['train', 'validation']:\n",
    "    ax.plot(metrics[key]['loss'], label=f'{key} loss')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd3a40-7508-4537-b76a-26434574df39",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840eb22-84ed-4abc-9842-1dba499b971e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "from viz_utils.make_video import make_comparison_grid, make_grid_video_with_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabdc3c-3826-484c-a4c0-03cefe8ffa05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = next(iter(val_dataset))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d09417-54e4-4839-acee-027c018fa2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene_rgb = sample['frames']\n",
    "scene_modal_mask = sample['modal_masks']\n",
    "gt_amodal_rgb = sample['amodal_content']\n",
    "gt_amodal_mask = sample['amodal_masks']\n",
    "pred_amodal_rgb = sample['amodal_content']\n",
    "pred_amodal_mask = sample['amodal_masks']\n",
    "n_frames=8 \n",
    "\n",
    "\n",
    "grid_tensors = make_comparison_grid(scene_rgb=scene_rgb, \n",
    "                                    scene_modal_mask=scene_modal_mask,\n",
    "                                    gt_amodal_rgb=gt_amodal_rgb, \n",
    "                                    gt_amodal_mask=gt_amodal_mask,\n",
    "                                    pred_amodal_rgb=pred_amodal_rgb, \n",
    "                                    pred_amodal_mask=pred_amodal_mask,\n",
    "                                    n_frames=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d508e-1c0b-4273-bf0c-6ac2de03103b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_grid_video_with_titles(grid_tensors,\n",
    "                            interval=200, save_path=\"/p/lustre2/marcou1/dsc2025/test_videos/test_sample_video.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbfaaa-5f7a-47f3-b46e-fc5a59761e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce8442-3d67-4f81-b36a-b0934ec3d2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run model forward with one sample\n",
    "# Fake up a batch with bs =1\n",
    "# Model forward expects\n",
    "# frames: (bs, c, n_frames, h, w)\n",
    "# modal_masks: (bs, 1, n_frames, h, w)\n",
    "input_batch = {'frames': sample['frames'].unsqueeze(0),\n",
    "              'modal_masks': sample['modal_masks'].unsqueeze(0),\n",
    "               'amodal_masks': sample['amodal_masks'].unsqueeze(0), \n",
    "               'amodal_content': sample['amodal_content'].unsqueeze(0)}\n",
    "\n",
    "for key, entry in input_batch.items():\n",
    "    print(key, entry.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1fd4b-57e7-4e5f-b1ff-0a809b73764f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run inference\n",
    "def run_inference_single_sample(model, batch, device):\n",
    "    \"\"\"\n",
    "    Run single sample UnetImage_ConvLSTM inference\n",
    "    \"\"\"\n",
    "    # First things first\n",
    "    # disable gradient tracking\n",
    "    with torch.no_grad():\n",
    "        batch = batch_to_device(batch, device)\n",
    "        # This takes in a batch - bs =1 for single sample\n",
    "        # RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "        # modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "        # concat rgb and modal masks as input to model\n",
    "\n",
    "        # frames: (bs, c, n_frames, h, w)\n",
    "        # modal_masks: (bs, 1, n_frames, h, w)\n",
    "        frames = batch['frames']         # (bs, c, n_frames, h, w)\n",
    "        modal_masks = batch['modal_masks']  # (bs, 1, n_frames, h, w)\n",
    "        frames = modal_masks * frames\n",
    "        # bs, c, n, h, w = frames.shape\n",
    "\n",
    "\n",
    "        # # Concatenate along channel dim: (bs, c+1, n_frames, h, w)\n",
    "        # model_input = torch.cat((frames, modal_masks), dim=1)\n",
    "\n",
    "        # This time we don't squeeze the time dimension out\n",
    "        # Prep for loss\n",
    "        # This is what we use for loss calculations\n",
    "        # Remember these are bs, c, n, h, w!\n",
    "        # These both have range 0 to 1 - same to logits\n",
    "        amodal_mask_labels = batch['amodal_masks'].float()\n",
    "        amodal_content_labels = batch['amodal_content'].float()\n",
    "\n",
    "        # Implementation here\n",
    "        # # Concatenate along channel dim: (bs, c+1, n_frames, h, w)\n",
    "        # model_input = torch.cat((frames, modal_masks), dim=1)\n",
    "        x = torch.cat([frames, modal_masks], dim=1)  # (bs, 4, n_frames, h, w)\n",
    "        bs, c, n_frames, h, w = x.shape\n",
    "        # Unet_Image encode expects  (bs, 4, n_frames, h, w)\n",
    "        # Permutation takes (bs, c, n_frames, h, w) --> (bs, n_frames, c, h, w)\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # (bs, n_frames, c, h, w) --> LSTM expects this\n",
    "\n",
    "        # Encode frames\n",
    "        x1_seq, x2_seq, x3_seq, x4_seq = model.encode_frames(x, n_frames)\n",
    "\n",
    "        # Pass bottleneck (x4_seq) through ConvLSTM\n",
    "        # Last layer should match dims from encode: torch.Size([6, 256, 1, 16, 16])]\n",
    "        # ConvLSTM expects B, T, C, H, W\n",
    "        convlstm_out_list, _ = model.convlstm(x4_seq)  # List of outputs for each layer\n",
    "        # Returns layer_output_list, last_state_list\n",
    "        # We only care about the layer output list\n",
    "        convlstm_out = convlstm_out_list[0]           # (bs, n_frames, channels, h, w)\n",
    "\n",
    "        # Decoding frames\n",
    "        # Pass in the sequences of frames and the output of the LSTM\n",
    "        # Will use the last frame for decoding\n",
    "        logits_amodal_mask, logits_amodal_content = model.decode_frames(x1_seq, x2_seq, x3_seq, convlstm_out, n_frames)\n",
    "\n",
    "        # Reshape logits - use permute\n",
    "        # They come out as bs, n_frames, c, h, w\n",
    "        # example:\n",
    "        # torch.Size([8, 8, 3, 256, 256]) torch.Size([8, 8, 1, 256, 256])\n",
    "        # Batch ground truth is: (bs, c, n_frames, h, w)\n",
    "        # Example\n",
    "        # torch.Size([8, 3, 8, 256, 256]) torch.Size([8, 1, 8, 256, 256])\n",
    "        logits_amodal_mask = logits_amodal_mask.permute(0, 2, 1, 3, 4)\n",
    "        logits_amodal_content = logits_amodal_content.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if model.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        mask_loss, content_loss = model.loss_function(logits_amodal_mask, \n",
    "                                                        amodal_mask_labels,\n",
    "                                                        logits_amodal_content, \n",
    "                                                        amodal_content_labels)\n",
    "\n",
    "        loss = mask_loss + content_loss\n",
    "\n",
    "        # Compute accuracy\n",
    "        # If preds are probabilities/logits, binarize them:\n",
    "        preds_mask = logits_amodal_mask.sigmoid().round() # make binary\n",
    "        true_mask = amodal_mask_labels.round() # make these binary as well\n",
    "        # Ensure labels are also float or int for comparison\n",
    "        # Calculate number of correct pixels\n",
    "        correct = (preds_mask == true_mask).float().sum()\n",
    "        # Calculate total number of pixels (numel = number of total elements)\n",
    "        total = torch.numel(preds_mask)\n",
    "        # Accuracy\n",
    "        accuracy = correct / total # this is a number 0 to 1\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "\n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "            'loss': loss.data.item(),\n",
    "            'mask_loss': mask_loss.data.item(),\n",
    "            'content_loss': content_loss.data.item(),\n",
    "            'accuracy': accuracy # add more metrics here - just make sure they are a number\n",
    "        }\n",
    "        \n",
    "    return loss, metrics, logits_amodal_mask, logits_amodal_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35780c3a-c2ed-4e22-b735-8a42d9b94a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, metrics, logits_amodal_mask, logits_amodal_content = run_inference_single_sample(model=model, batch=input_batch, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabea8e-790e-44a5-8f0f-bbf85de0746f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"mask\", logits_amodal_mask.min(), logits_amodal_mask.max())\n",
    "print(\"content\", logits_amodal_content.min(), logits_amodal_content.max())\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60314ed-4f43-4fdb-a01f-9610587d8eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits_amodal_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeecf2-c4ec-4b25-83a7-8730ad17e116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(gt_amodal_rgb.shape, gt_amodal_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dbec1-10cb-4f07-96b3-ec761144d111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_output_to_sample_video_input_format(logits_amodal_content, logits_amodal_mask, true_amodal_content, true_amodal_mask):\n",
    "    \"\"\"\n",
    "    Convert single sample Unet evaluation (logits), to plotting ready numpy arrays\n",
    "    Unet_Image ground truth: true_amodal_content, true_amodal_mask\n",
    "\n",
    "    Args:\n",
    "        logits_amodal_content: torch.tensor, [bs, c, n_frames, h, w]\n",
    "        logits_amodal_mask: torch.tensor [bs, 1, c, n_frames, h, w]\n",
    "        true_amodal_content: torch.tensor [c, n_frames, h, w]\n",
    "        true_amodal_mask: torch.tensor [c, n_frames, h, w]\n",
    "\n",
    "    Returns:\n",
    "        preds_amodal_content: torch.tensor [c, n_frames, h, w]\n",
    "        preds_amodal_mask: torch.tensor [c, n_frames, h, w]\n",
    "    \"\"\"\n",
    "    # 1. RGB Content\n",
    "    # print(\"AMODAL RGB CONTENT\")\n",
    "    # print(\"Logits amodal content, initial (after masking): logits amodal content shape\", logits_amodal_content.shape, \n",
    "    #     \"range\", logits_amodal_content.min(), logits_amodal_content.max())\n",
    "\n",
    "    # These are masked amodal contents for the object\n",
    "    # Apply sigmoid to logits for amodal content (no rounding!)\n",
    "    logits_amodal_content = logits_amodal_content.sigmoid()\n",
    "    # print(\"Logits amodal content, masked and sigmoid applied: logits amodal content shape\", logits_amodal_content.shape, \n",
    "    #     \"range\", logits_amodal_content.min(), logits_amodal_content.max())\n",
    "\n",
    "    # Squeeze to remove the batch dimensions\n",
    "    preds_amodal_content = logits_amodal_content.squeeze(0)\n",
    "    # print(\"Preds Amodal RGB, after squeeze: preds_amodal_content shape\", \n",
    "    #     preds_amodal_content.shape, \"range\", preds_amodal_content.min(), preds_amodal_content.max())\n",
    "\n",
    "\n",
    "    preds_amodal_content = preds_amodal_content.cpu()\n",
    "\n",
    "    # Apply clip\n",
    "    preds_amodal_content = torch.clamp(preds_amodal_content, min=0, max=1)\n",
    "\n",
    "    # Multiply by 255 to get color range\n",
    "    preds_amodal_content = (preds_amodal_content * 255).to(torch.uint8)     # [0, 255], uint8\n",
    "    # print(\"Preds Amodal RGB, after multiply and int: preds_amodal_content shape\", \n",
    "    #     preds_amodal_content.shape, \"range\", preds_amodal_content.min(), preds_amodal_content.max())\n",
    "\n",
    "    print(f\"AMODAL CONTENT: true {true_amodal_content.shape}, pred {preds_amodal_content.shape}\")\n",
    "    print(\"range comparison\\n\", \n",
    "        f\"true {true_amodal_content.min()}, {true_amodal_content.max()}\\n\",\n",
    "        f\"preds {preds_amodal_content.min()}, {preds_amodal_content.max()}\\n\")\n",
    "\n",
    "    # check shape\n",
    "    assert preds_amodal_content.shape == true_amodal_content.shape\n",
    "\n",
    "\n",
    "    # 2. Amodal Mask\n",
    "    # print(\"AMODAL RGB MASK\")\n",
    "\n",
    "    # print(\"Logits amodal mask, initial: shape\", logits_amodal_mask.shape, \n",
    "    #     \"range\", logits_amodal_mask.min(), logits_amodal_mask.max())\n",
    "\n",
    "    # Apply rounded sigmoids to get integers, binary\n",
    "    preds_amodal_mask = logits_amodal_mask.sigmoid().round().to(torch.uint8)\n",
    "    # This yields bs, c, h, w\n",
    "    # print(\"Preds amodal mask, after sigmoid: shape\", preds_amodal_mask.shape, \n",
    "    #     \"range\", preds_amodal_mask.min(), preds_amodal_mask.max())\n",
    "\n",
    "    # Drop ix = 0 (batch dimension)\n",
    "    preds_amodal_mask = preds_amodal_mask.squeeze(0)            # [1, H, W]\n",
    "    # print(\"Preds amodal mask, after dropping bs: shape\", preds_amodal_mask.shape, \n",
    "    #     \"range\", preds_amodal_mask.min(), preds_amodal_mask.max())\n",
    "\n",
    "    preds_amodal_mask = preds_amodal_mask.cpu()    # For probability map (grayscale)\n",
    "\n",
    "\n",
    "    print(f\"AMODAL MASK: true {true_amodal_mask.shape}, pred {preds_amodal_mask.shape}\")\n",
    "    print(\"range comparison\\n\", \n",
    "        f\"true {true_amodal_mask.min()}, {true_amodal_mask.max()}\\n\",\n",
    "        f\"preds {preds_amodal_mask.min()}, {preds_amodal_mask.max()}\\n\")\n",
    "\n",
    "    # check shape\n",
    "    assert preds_amodal_content.shape == true_amodal_content.shape\n",
    "\n",
    "\n",
    "    return preds_amodal_content, preds_amodal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfca07-d194-4b06-bf91-2bf9dac96f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_amodal_content, preds_amodal_mask = convert_output_to_sample_video_input_format(logits_amodal_content=logits_amodal_content,\n",
    "                                                                                      logits_amodal_mask=logits_amodal_mask,\n",
    "                                                                                      true_amodal_content=gt_amodal_rgb,\n",
    "                                                                                      true_amodal_mask=gt_amodal_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36879a8c-9355-46f3-927a-6e9dadf4b1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene_rgb = sample['frames']\n",
    "scene_modal_mask = sample['modal_masks']\n",
    "scene_rgb = sample['frames'] * scene_modal_mask\n",
    "gt_amodal_rgb = sample['amodal_content']\n",
    "gt_amodal_mask = sample['amodal_masks']\n",
    "pred_amodal_rgb = preds_amodal_content\n",
    "pred_amodal_mask = preds_amodal_mask\n",
    "n_frames=8 \n",
    "\n",
    "grid_tensors = make_comparison_grid(scene_rgb=scene_rgb, \n",
    "                                    scene_modal_mask=scene_modal_mask,\n",
    "                                    gt_amodal_rgb=gt_amodal_rgb,\n",
    "                                    gt_amodal_mask=gt_amodal_mask,\n",
    "                                    pred_amodal_rgb=pred_amodal_rgb,\n",
    "                                    pred_amodal_mask=preds_amodal_mask,\n",
    "                                    n_frames=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72856f86-0626-4450-ad1d-08a5a35e4cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4788c-d5be-4cfd-a532-ea52af2558d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make video\n",
    "make_grid_video_with_titles(grid_tensors,\n",
    "                            interval=200, \n",
    "                            save_path=\"/p/lustre2/marcou1/dsc2025/test_videos/test_sample_inference_{}_{}_{}.gif\".format(sample['metadata']['scene'],\n",
    "                                                                                                                  sample['metadata']['cam_id'],\n",
    "                                                                                                                   sample['metadata']['obj_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59656d-08a5-4f8c-a9de-9e0cf2330437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscenv",
   "language": "python",
   "name": "dsc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
