{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0afcc41",
   "metadata": {},
   "source": [
    "# Task 2.1\n",
    "\n",
    "“Given a video of the modal mask of an object, predict the amodal mask of the same object”\n",
    "\n",
    "Inputs:\n",
    "- RGB Frames - N 3-channel images (N, 3, 256, 256)\n",
    "- N Modal Masks of Object i - N Binary (1-channel) images (N, 1, 256, 256)\n",
    "\n",
    "Outputs:\n",
    "- N Amodal Masks of Object i - N Binary (1-channel) images (1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cfba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch, Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import write_video\n",
    "\n",
    "# Common\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from IPython.display import Video\n",
    "\n",
    "# Utils from Torchvision\n",
    "tensor_to_image = ToPILImage()\n",
    "image_to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e55d15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51d768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f682926b",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795482dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(amodal_mask_preds, \n",
    "                      amodal_mask_labels, \n",
    "                      amodal_content_preds,\n",
    "                      amodal_content_labels):\n",
    "    \"\"\"\n",
    "    Here, you can calculate non-loss metrics like mIOU, accuracy, J&F scores.\n",
    "\n",
    "    And non-loss image generation metrics between the predicted and ground-truth amodal content\n",
    "    Such as Inception Score, Frechet Inception Distance, Learned Perceptual Patch Similarity (LPIPS),\n",
    "    Structure Similarity Index Metric (SSIM), Peak Signal-Noise Ratio (PSNR)\n",
    "\n",
    "    These should all have easy-to-use implementations in libraries such as TorchMetrics.\n",
    "    \"\"\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176772e2",
   "metadata": {},
   "source": [
    "#### Class - Unet_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9771b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class conv2d_inplace_spatial(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, pooling_function, activation = nn.GELU()):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation,\n",
    "            pooling_function,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, scale_factor=(2, 2), mode='bilinear', align_corners=False):\n",
    "        super(Upscale, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
    "\n",
    "class Unet_Image(nn.Module):\n",
    "    def __init__(self, in_channels = 4, mask_content_preds = False):\n",
    "        super().__init__()\n",
    "        # downsampling\n",
    "        self.mpool_2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        # down path - encoding - increase dimensions\n",
    "        self.down1 = conv2d_inplace_spatial(in_channels, 32, self.mpool_2)\n",
    "        self.down2 = conv2d_inplace_spatial(32, 64, self.mpool_2)\n",
    "        self.down3 = conv2d_inplace_spatial(64, 128, self.mpool_2)\n",
    "        self.down4 = conv2d_inplace_spatial(128, 256, self.mpool_2)\n",
    "        \n",
    "        # upsample by factor of 2, in both dimensions\n",
    "        self.upscale_2 = Upscale(scale_factor=(2, 2), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # up path - decoder\n",
    "        # slowly reduce dimensions\n",
    "        self.up1 = conv2d_inplace_spatial(256, 128, self.upscale_2)\n",
    "        self.up2 = conv2d_inplace_spatial(256, 64, self.upscale_2)\n",
    "        self.up3 = conv2d_inplace_spatial(128, 32, self.upscale_2)\n",
    "        \n",
    "        # no activation\n",
    "        self.up4_amodal_mask = conv2d_inplace_spatial(64, 1, self.upscale_2, activation = nn.Identity())\n",
    "        self.up4_amodal_content = conv2d_inplace_spatial(64, 3, self.upscale_2, activation = nn.Identity())\n",
    "\n",
    "        # Optional arguments\n",
    "        self.mask_content_preds = mask_content_preds # Should we mask the amodal content prediction by the amodal mask prediction?\n",
    "\n",
    "        # Optimization\n",
    "        self.mse_loss = nn.L1Loss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "        # Self-attention feature enrichment\n",
    "        #max_seq = 16*16*6\n",
    "        #token_dim = 256\n",
    "        #self.pos_enc = nn.Parameter(torch.zeros((max_seq, 1, token_dim))) # seq b dim\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=token_dim, nhead=8)\n",
    "        #self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "    \n",
    "    # Packages the encoder path (downpath)\n",
    "    def encode(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        \n",
    "        # Returns the four\n",
    "        # [torch.Size([6, 32, 4, 128, 128]), torch.Size([6, 64, 2, 64, 64]), torch.Size([6, 128, 1, 32, 32]), torch.Size([6, 256, 1, 16, 16])]\n",
    "        return x1, x2, x3, x4\n",
    "    \n",
    "    # packages the decode path (up-path)\n",
    "    def decode(self, h1, h2, h3, h4):\n",
    "        h4 = self.up1(h4) # 6, 256, 1, 16, 16 -> 6, 128, 1, 32, 32 (double spatial, then conv-in-place channels to half)\n",
    "        h34 = torch.cat((h3, h4), dim = 1) # (6, 2*128, 1, 32, 32)\n",
    "\n",
    "        h34 = self.up2(h34) # 6, 256, 1, 32, 32 -> 6, 128, 2, 64, 64\n",
    "        h234 = torch.cat((h2, h34), dim = 1)\n",
    "\n",
    "        h234 = self.up3(h234)\n",
    "        h1234 = torch.cat((h1, h234), dim = 1)\n",
    "        \n",
    "        logits_amodal_mask = self.up4_amodal_mask(h1234)\n",
    "        logits_amodal_content = self.up4_amodal_content(h1234)\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "    \n",
    "    # Encode - decode together\n",
    "    # Runs through the whole model\n",
    "    def encode_decode(self, x):\n",
    "        \"\"\"\n",
    "        input image tensor: (bs, c, h, w) - batch size, channels, height, width\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # Multiscale features x1, x2, x3, x4\n",
    "        x1, x2, x3, x4 = self.encode(x)\n",
    "\n",
    "        # You could add code here for example more layers that modify the latent x4? Be creative :)\n",
    "\n",
    "        # Decode using enriched features\n",
    "        logits_amodal_mask, logits_amodal_content = self.decode(x1, x2, x3, x4)\n",
    "\n",
    "        return logits_amodal_mask, logits_amodal_content\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # This takes in a batch\n",
    "        # RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "        # modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "        # concat rgb and modal masks as input to model\n",
    "        model_input = torch.cat((batch['frames'], batch['modal_masks']), dim = 1)\n",
    "\n",
    "        # Remove time dimension from a few things (add it back in later when you make your video model!)\n",
    "        model_input = model_input.squeeze(2) # remove time dimension (you will probably want it later!)\n",
    "        # This is what we use for loss calculations\n",
    "        # These both have range 0 to 1 - same to logits\n",
    "        amodal_mask_labels = batch['amodal_masks'].float().squeeze(2)\n",
    "        amodal_content_labels = batch['amodal_content'].float().squeeze(2)\n",
    "\n",
    "        # Model input\n",
    "        #print('model input:', model_input.shape)\n",
    "        \n",
    "        logits_amodal_mask, logits_amodal_content = self.encode_decode(model_input)\n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if self.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        mask_loss, content_loss = self.loss_function(logits_amodal_mask, \n",
    "                                                        amodal_mask_labels,\n",
    "                                                        logits_amodal_content, \n",
    "                                                        amodal_content_labels)\n",
    "        \n",
    "        loss = mask_loss + content_loss\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "        \n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "            'loss': loss.data.item(),\n",
    "            'mask_loss': mask_loss.data.item(),\n",
    "            'content_loss': content_loss.data.item(),\n",
    "            'other_metrics_to_monitor': 0 # add more metrics here - just make sure they are a number\n",
    "        }\n",
    "        return loss, metrics\n",
    "    def loss_function(self,\n",
    "                    amodal_mask_preds,\n",
    "                    amodal_mask_labels,\n",
    "                    amodal_content_preds,\n",
    "                    amodal_content_labels):\n",
    "        # Compares raw logits for binary prediction - object vs no object\n",
    "        mask_loss = self.bce_loss(amodal_mask_preds, amodal_mask_labels) # Binary Cross Entropy Loss\n",
    "        # Pixel by pixel difference in the RGB\n",
    "        content_loss = self.mse_loss(amodal_content_preds, amodal_content_labels) # L1 loss\n",
    "        return mask_loss, content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a285f",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "\n",
    "Test model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71a540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: torch.Size([1, 1, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Make a U-net which expects a batch of 4-channel images as input (batch_size, channels=4, height, width)\n",
    "model = Unet_Image(4)\n",
    "\n",
    "# Make a dummy tensor to test the model can successfully apply its layers and get an output\n",
    "batch_size = 1\n",
    "rand_input = torch.randn((batch_size, 4, 256, 256))\n",
    "\n",
    "# Apply the model to the input - we use encode decode here rather than forward\n",
    "# because we don't have the full batch yet - we will later\n",
    "logits_amodal_mask, logits_amodal_content = model.encode_decode(rand_input)\n",
    "print('Model output:', logits_amodal_mask.shape, logits_amodal_content.shape)\n",
    "\n",
    "# For our use-case, we are predicting amodal masks and amodal content\n",
    "# The amodal mask prediction should be (batch_size, 1, h, w) for a binary mask and the \n",
    "# Amodal content prediction should be  (batch_size, 3, h, w) for an rgb object prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa2cf7",
   "metadata": {},
   "source": [
    "#### Class - Unet_Image + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8500aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_Image_LSTM(Unet_Image):\n",
    "    def __init__(self, in_channels=4, mask_content_preds=False, input_size=(128, 128)):\n",
    "        super().__init__(in_channels, mask_content_preds)\n",
    "        # Dynamically infer bottleneck shape\n",
    "        dummy = torch.zeros(1, in_channels, *input_size)\n",
    "        with torch.no_grad():\n",
    "            _, _, _, x4 = self.encode(dummy)\n",
    "        _, bottleneck_channels, h_b, w_b = x4.shape\n",
    "        self.bottleneck_channels = bottleneck_channels\n",
    "        self.bottleneck_h = h_b\n",
    "        self.bottleneck_w = w_b\n",
    "        self.lstm_input_size = bottleneck_channels * h_b * w_b\n",
    "        self.lstm_hidden_size = self.lstm_input_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_size,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "    def encode_frames(self, x, n):\n",
    "        x1_list, x2_list, x3_list, x4_list = [], [], [], []\n",
    "        for t in range(n): # n gives how many frames to expect\n",
    "            frame_t = x[:, :, t, :, :]  # (bs, c+1, h, w), same shape as the inputs of Unet_Image\n",
    "            x1, x2, x3, x4 = self.encode(frame_t)\n",
    "            x1_list.append(x1)\n",
    "            x2_list.append(x2)\n",
    "            x3_list.append(x3)\n",
    "            x4_list.append(x4)\n",
    "        # Stack along time: (bs, cX, n_frames, hX, wX)\n",
    "        x1_seq = torch.stack(x1_list, dim=2)\n",
    "        x2_seq = torch.stack(x2_list, dim=2)\n",
    "        x3_seq = torch.stack(x3_list, dim=2)\n",
    "        x4_seq = torch.stack(x4_list, dim=2)\n",
    "        return x1_seq, x2_seq, x3_seq, x4_seq\n",
    "    \n",
    "    def run_lstm(self, h4_seq):\n",
    "        # Uses the last layer of encoder output\n",
    "        # Prepare LSTM input: flatten spatial dims\n",
    "        bs, ch, n, h_b, w_b = h4_seq.shape\n",
    "        h4_seq_flat = h4_seq.permute(0, 2, 1, 3, 4).contiguous().view(bs, n, -1)  # (bs, n, ch*h_b*w_b)\n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(h4_seq_flat)  # (bs, n, hidden_size)\n",
    "        # Reshape LSTM output back to spatial, then permute back to (bs, ch, n, h, w)\n",
    "        lstm_out_spatial = lstm_out.view(bs, n, ch, h_b, w_b).permute(0, 2, 1, 3, 4)\n",
    "        return lstm_out_spatial\n",
    "    \n",
    "    def decode_frames(self, h1_seq, h2_seq, h3_seq, h4_seq, n):\n",
    "        # Decode each timestep\n",
    "        logits_mask_seq = []\n",
    "        logits_content_seq = []\n",
    "        for t in range(n): # n is the number of consecutive frames\n",
    "            # Picks a single entry of the time dimension resulting in (bs, c, h, w) shape\n",
    "            logits_mask, logits_content = self.decode(\n",
    "                h1_seq[:, :, t, :, :],\n",
    "                h2_seq[:, :, t, :, :],\n",
    "                h3_seq[:, :, t, :, :],\n",
    "                h4_seq[:, :, t, :, :]\n",
    "            )\n",
    "            logits_mask_seq.append(logits_mask)\n",
    "            logits_content_seq.append(logits_content)\n",
    "        # Stack outputs along time (dim=2)\n",
    "        logits_mask_seq = torch.stack(logits_mask_seq, dim=2)      # (bs, 1, n_frames, h, w)\n",
    "        logits_content_seq = torch.stack(logits_content_seq, dim=2) # (bs, 3, n_frames, h, w)\n",
    "        return logits_mask_seq, logits_content_seq\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # This takes in a batch\n",
    "        # RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "        # modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "        # concat rgb and modal masks as input to model\n",
    "\n",
    "        # frames: (bs, c, n_frames, h, w)\n",
    "        # modal_masks: (bs, 1, n_frames, h, w)\n",
    "        frames = batch['frames']         # (bs, c, n_frames, h, w)\n",
    "        modal_masks = batch['modal_masks']  # (bs, 1, n_frames, h, w)\n",
    "        bs, c, n, h, w = frames.shape\n",
    "\n",
    "        # Concatenate along channel dim: (bs, c+1, n_frames, h, w)\n",
    "        model_input = torch.cat((frames, modal_masks), dim=1)\n",
    "\n",
    "        # This time we don't squeeze the time dimension out\n",
    "        # Prep for loss\n",
    "        # This is what we use for loss calculations\n",
    "        # These both have range 0 to 1 - same to logits\n",
    "        amodal_mask_labels = batch['amodal_masks'].float().squeeze(2)\n",
    "        amodal_content_labels = batch['amodal_content'].float().squeeze(2)\n",
    "\n",
    "        # Process each frame in the sequence\n",
    "        # Pass this into the encoder function\n",
    "        # Returns the four\n",
    "        # [torch.Size([6, 32, 4, 128, 128]), torch.Size([6, 64, 2, 64, 64]), torch.Size([6, 128, 1, 32, 32]), torch.Size([6, 256, 1, 16, 16])]\n",
    "        x1_seq, x2_seq, x3_seq, x4_seq = self.encode_frames(x=model_input, n=n)\n",
    "\n",
    "        # Run LSTM\n",
    "        lstm_out_spatial = self.run_lstm(x4_seq)        \n",
    "\n",
    "        # Run decoding of frames\n",
    "        logits_amodal_mask, logits_amodal_content = self.decode_frames(x1_seq, x2_seq, x3_seq, lstm_out_spatial, n=n)\n",
    "        \n",
    "\n",
    "        # Should we mask the amodal content prediction by the predicted amodal mask?\n",
    "        if self.mask_content_preds:\n",
    "            # Element-wise masking by self-predictions:\n",
    "            logits_amodal_content = logits_amodal_mask.sigmoid().round() * logits_amodal_content\n",
    "\n",
    "        # print('0000 Verify Shapes 0000')\n",
    "        # print(logits_amodal_mask.shape)\n",
    "        # print(logits_amodal_content.shape)\n",
    "        # print(amodal_mask_labels.shape)\n",
    "        # print(amodal_content_labels.shape)\n",
    "        # Loss between predicted amodal masks/content and GT masks/content\n",
    "        mask_loss, content_loss = self.loss_function(logits_amodal_mask, \n",
    "                                                        amodal_mask_labels,\n",
    "                                                        logits_amodal_content, \n",
    "                                                        amodal_content_labels)\n",
    "        \n",
    "        loss = mask_loss + content_loss\n",
    "\n",
    "        # Calculate metrics into a dictionary\n",
    "        metrics = calculate_metrics(logits_amodal_mask, \n",
    "                                    amodal_mask_labels, \n",
    "                                    logits_amodal_content,\n",
    "                                    amodal_content_labels)\n",
    "        \n",
    "        # Report the metrics we calculated in addition to our loss functions\n",
    "        metrics = {\n",
    "            'loss': loss.data.item(),\n",
    "            'mask_loss': mask_loss.data.item(),\n",
    "            'content_loss': content_loss.data.item(),\n",
    "            'other_metrics_to_monitor': 0 # add more metrics here - just make sure they are a number\n",
    "        }\n",
    "        return loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8148d0",
   "metadata": {},
   "source": [
    "#### Put dummies through new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda33449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a U-net which expects a batch of 4-channel images as input (batch_size, channels=4, height, width)\n",
    "model = Unet_Image_LSTM(in_channels=4, mask_content_preds=True)\n",
    "\n",
    "# Make a dummy tensor to test the model can successfully apply its layers and get an output\n",
    "# RGB image batch of shape (bs, c, n_frames, h, w)\n",
    "# modal_mask (binary, c=1) batch of shape (bs, c, n_frames, h, w)\n",
    "# all in all batch is (bs, 4, n_frames, h, w)\n",
    "batch_size = 1\n",
    "rand_input = torch.randn((batch_size, 4, 8, 256, 256))\n",
    "bs, c, n, h, w = rand_input.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For our use-case, we are predicting amodal masks and amodal content\n",
    "# The amodal mask prediction should be (batch_size, 1, h, w) for a binary mask and the \n",
    "# Amodal content prediction should be  (batch_size, 3, h, w) for an rgb object prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3862ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test layer by layer\n",
    "with torch.no_grad():\n",
    "    x1_seq, x2_seq, x3_seq, x4_seq = model.encode_frames(x=rand_input, n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c66b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162674ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 8, 16, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd321a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run LSTM\n",
    "with torch.no_grad():\n",
    "    lstm_out_spatial = model.run_lstm(x4_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
