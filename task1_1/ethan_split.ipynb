{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch, Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "\n",
    "# Common\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For train-validation-test split (if we want to do it manually)\n",
    "import random\n",
    "from math import floor\n",
    "\n",
    "# Utils from Torchvision\n",
    "tensor_to_image = ToPILImage()\n",
    "image_to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f53e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions (from Amar)\n",
    "def get_img_dict(img_dir):\n",
    "    img_files = [x for x in img_dir.iterdir() if x.name.endswith('.png') or x.name.endswith('.tiff')]\n",
    "    img_files.sort()\n",
    "\n",
    "    img_dict = {}\n",
    "    for img_file in img_files:\n",
    "        img_type = img_file.name.split('_')[0] \n",
    "        if img_type not in img_dict:\n",
    "            img_dict[img_type] = []\n",
    "        img_dict[img_type].append(img_file)\n",
    "    return img_dict\n",
    "\n",
    "def get_sample_dict(sample_dir):\n",
    "\n",
    "    camera_dirs = [x for x in sample_dir.iterdir() if 'camera' in x.name]\n",
    "    camera_dirs.sort()\n",
    "    \n",
    "    sample_dict = {}\n",
    "\n",
    "    for cam_dir in camera_dirs:\n",
    "        cam_dict = {}\n",
    "        cam_dict['scene'] = get_img_dict(cam_dir)\n",
    "\n",
    "        obj_dirs = [x for x in cam_dir.iterdir() if 'obj_' in x.name]\n",
    "        obj_dirs.sort()\n",
    "        \n",
    "        for obj_dir in obj_dirs:\n",
    "            cam_dict[obj_dir.name] = get_img_dict(obj_dir)\n",
    "\n",
    "        sample_dict[cam_dir.name] = cam_dict\n",
    "\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76071a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset11(videos_dir: str, \n",
    "                          inds: list[int] | None = None, \n",
    "                          random_seed: int = 42) -> tuple[torch.tensor, torch.tensor, list[dict]]: \n",
    "    \"\"\" \n",
    "    Loads one frame for every object viewed from every camera angle in every video in videos_dir and \n",
    "    returns the modal masks and amodal masks in tensor form.\n",
    "\n",
    "    Parameters:\n",
    "        - videos_dir: a directory where each sub-directory contains a video from Movi-MC-AC\n",
    "        - inds: a list of the same length as the number of videos telling which frame of each video to load into the dataset\n",
    "        - random_seed: an integer to set the random seed. Only necessary if inds is None.\n",
    "\n",
    "    Returns: \n",
    "        - a tensor of modal masks\n",
    "        - a tensor of amodal masks\n",
    "        - a list of ALL images from ALL videos. TODO: abstract this into a separate function\n",
    "        - TODO: also return a tensor of RGB images\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the RNG state to restore it after we're done\n",
    "    prev_state = random.getstate()\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    # If user does not specify, take one random frame from each video\n",
    "    if inds is None: \n",
    "        inds = [random.randint(0, num_frames - 1) for i in range(num_vids)]\n",
    "    random.setstate(prev_state)\n",
    "\n",
    "    # Load a dictionary storing each video file\n",
    "    videos_dir = Path(videos_dir)\n",
    "    video_dicts = [get_sample_dict(Path(sample_dir)) for sample_dir in videos_dir.iterdir()]\n",
    "\n",
    "    num_vids = len(video_dicts)\n",
    "\n",
    "    # Count number of objects in each video\n",
    "    num_obj_list = [None] * num_vids \n",
    "    for i in range(num_vids):\n",
    "        num_obj_list[i] = sum([s.startswith('obj') for s in video_dicts[i]['camera_0000'].keys()])\n",
    "\n",
    "    num_obj = sum(num_obj_list)\n",
    "    \n",
    "    # Every video in Movi-MC-AC has 24 frames and 6 camera angles\n",
    "    num_cams = 6 \n",
    "    num_frames = 24 \n",
    "\n",
    "    # Sample size\n",
    "    dataset_len = num_vids * num_cams * num_obj\n",
    "    \n",
    "    modal_tensor = torch.empty((dataset_len, 256, 256))\n",
    "    amodal_tensor = torch.empty((dataset_len, 256, 256))\n",
    "\n",
    "    l = 0 # iterates through l = 0, ..., dataset_len - 1\n",
    "    for i in range(num_vids): # for each video\n",
    "        video_dict = video_dicts[i]\n",
    "        cur_num_obj = num_obj_list[i]\n",
    "        for j in range(num_cams): # for each camera angle\n",
    "            cam_dict = video_dict[f'camera_{j:04d}']\n",
    "            modal_masks = Image.open(cam_dict['scene']['segmentation'][inds[i]]) # load the modal masks\n",
    "            for k in range(cur_num_obj): # for each object\n",
    "                amodal_mask = Image.open(cam_dict[f'obj_{k+1:04d}']['segmentation'][inds[i]]) # load the amodal mask\n",
    "                \n",
    "                modal_mask = (torch.tensor(np.array(modal_masks)) == (k + 1)).float() # extract the modal mask of a single object\n",
    "                amodal_mask = image_to_tensor(amodal_mask)\n",
    "\n",
    "                modal_tensor[l,:,:] = modal_mask \n",
    "                amodal_tensor[l,:,:] = amodal_mask\n",
    "                l += 1\n",
    "    \n",
    "    return modal_tensor, amodal_tensor, video_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db236a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can put the sample data from the demo (the folder starting with ff...) in a \n",
    "# directory called `data` to test this\n",
    "modal_tensor, amodal_tensor, video_dicts = make_dataset11('../data', \n",
    "                                                          [1]) # take the first frame of the video\n",
    "\n",
    "# Display one set of modal & amodal masks\n",
    "# Create a figure with 2 columns\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# Plot the images\n",
    "axes[0].imshow(tensor_to_image(modal_tensor[8]), cmap='gray')  # First image\n",
    "axes[0].set_title(\"Modal Mask\")\n",
    "axes[0].axis('off')  # Turn off axes\n",
    "\n",
    "axes[1].imshow(tensor_to_image(amodal_tensor[8]), cmap='gray')  # Second image\n",
    "axes[1].set_title(\"Amodal Mask\")\n",
    "axes[1].axis('off')  # Turn off axes\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X: torch.tensor, \n",
    "                         Y: torch.tensor, \n",
    "                         props: tuple[float, float, float], \n",
    "                         random_seed: int = 42) -> list[torch.tensor]:\n",
    "    ''' \n",
    "    Split two tensors, X and Y, into train, validation, and test datasets according to proportions `probs`.\n",
    "\n",
    "    Parameters:\n",
    "        - X: input data\n",
    "        - Y: output data\n",
    "        - props: a length 3 tuple which must sum to 1. probs = (0.7, 0.2, 0.1) specifies 70% of the data \n",
    "        in training, 20% of the data in validation, and 10% in testing. Any of these three percentages can \n",
    "        be 0.\n",
    "        - random_seed: Integer to set the RNG state.\n",
    "    \n",
    "    Return:\n",
    "        - 6 tensors: X and Y split into train, validation, and test.\n",
    "\n",
    "    Details:\n",
    "    \n",
    "    Validation data should be used for hyperparameter tuning and model selection, while test data should \n",
    "    be used for model evaluation (to make sure the model isn't under or overfitting).\n",
    "    '''\n",
    "    # Make sure proportions sum to 1, within floating point error\n",
    "    if (abs(sum(props) - 1)) >= 1e-4: \n",
    "        raise ValueError(\"props must sum to 1\")\n",
    "\n",
    "    if X.shape[0] != Y.shape[0]: \n",
    "        raise ValueError(\"X and Y must have the same sample size.\")\n",
    "    \n",
    "    n = X.shape[0]\n",
    "\n",
    "    test_size = floor(n * props[2])\n",
    "\n",
    "    # Make sure test size is not accidentally rounded to 0\n",
    "    if props[2] > 1e-4:\n",
    "        test_size = max(test_size, 1)\n",
    "\n",
    "    # Make sure validation size is not accidentally rounded to 0\n",
    "    val_size = floor(n * props[1])\n",
    "    if props[1] > 1e-4:\n",
    "        val_size = max(val_size, 1)\n",
    "\n",
    "    data_inds = range(n)\n",
    "\n",
    "    prev_state = random.getstate()\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    test_inds = random.sample(data_inds, test_size)\n",
    "    X_test = X[test_inds]\n",
    "    Y_test = Y[test_inds]\n",
    "\n",
    "    train_val_inds = set(data_inds) - set(test_inds)\n",
    "    val_inds = random.sample(list(train_val_inds), val_size)\n",
    "    X_val = X[val_inds]\n",
    "    Y_val = Y[val_inds]\n",
    "\n",
    "    train_inds = list(train_val_inds - set(val_inds))\n",
    "    random.shuffle(train_inds)\n",
    "\n",
    "    random.setstate(prev_state)\n",
    "\n",
    "    X_train = X[train_inds]\n",
    "    Y_train = Y[train_inds]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b316d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([701, 200, 200]),\n",
       " torch.Size([701, 200, 200]),\n",
       " torch.Size([200, 200, 200]),\n",
       " torch.Size([200, 200, 200]),\n",
       " torch.Size([100, 200, 200]),\n",
       " torch.Size([100, 200, 200]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of splitting empty tensors\n",
    "res = train_val_test_split(torch.empty((1001, 200, 200)), \n",
    "                           torch.empty((1001, 200, 200)), \n",
    "                           (0.7, 0.2, 0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
