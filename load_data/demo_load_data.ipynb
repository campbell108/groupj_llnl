{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b0f5b0-c9b2-40e5-8870-c7e6fbd99b6a",
   "metadata": {},
   "source": [
    "# Demo for using Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50650070-78ee-46a1-be2e-ad92c7b4bfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# local imports\n",
    "from load_dataset import MOVi_Dataset, MOVi_ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44840ed-596b-4c6a-957a-2883d6059e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec1f09-70d5-47b3-9c4e-11cb8fa6ab5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a11e260c-51f7-4336-85bd-e21c680b0b4a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a578b78-427e-4b39-b59b-9275f6da966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace with your root path\n",
    "ROOT_PATH = \"/p/lustre2/marcou1/dsc2025/dsc25_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a1963-338b-4521-b7d3-bf71962f23a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This loads in the video dataset - one object per sample, all frames\n",
    "movi_ds = MOVi_Dataset(root=ROOT_PATH, split = 'test', n_frames = 8, n_samples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b8bd7-5079-45d1-85a8-4b79df300da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check sample and dimensions\n",
    "sample = next(iter(movi_ds))\n",
    "print('frame', sample['frames'].shape) # 3 chans, 8 frames, 256x256 pixels\n",
    "print('depth', sample['depths'].shape) # 1 chans (float), 8 frames, 256x256 pixels\n",
    "print('mmasks',sample['modal_masks'].shape, sample['modal_masks'].dtype) # 1 chans, 8 frames, 256x256 pixels (int)\n",
    "print(sample['amodal_masks'].shape, sample['modal_masks'].dtype) # 1 chans, 8 frames, 256x256 pixels (int)\n",
    "print(sample['amodal_content'].shape, sample['amodal_content'].dtype) # 3 chans (float), 8 frames, 256x256 pixels (int)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f6f1a-02a1-40b2-b9b7-71704717c73c",
   "metadata": {},
   "source": [
    "## Batch and pass to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4cc38-1505-4817-87f4-0350881cb318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass to torch and batch\n",
    "dataloader = DataLoader(\n",
    "    movi_ds,\n",
    "    batch_size=4,     # Or whatever batch size you want\n",
    "    shuffle=True,     # Shuffle for training\n",
    "    num_workers=1     # Set >0 for faster loading if you have CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981ec8a-6642-4b71-a504-191641b27884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(iter(dataloader)) # this gives one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a43103-cf9c-42aa-a792-7105d2b13fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dataloader) # 8 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965f393-b537-4b19-9b0c-d50b656cccbf",
   "metadata": {},
   "source": [
    "### Iterate over batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebd860-2516-4494-a82b-a3f298d93b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over batches\n",
    "# EXAMPLE: Using RGB content (frames) and modal masks as input,\n",
    "# Amodal mask is output\n",
    "# This is a reduced form of Task 2.1 - not using the amodal content\n",
    "for batch in dataloader:\n",
    "    # Select features from the batch (B)\n",
    "    # Inputs\n",
    "    frames = batch['frames']         # [B, 3, n_frames, H, W]\n",
    "    modal_masks = batch['modal_masks']  # [B, 1, n_frames, H, W]\n",
    "\n",
    "    # Output (target)\n",
    "    amodal_masks = batch['amodal_masks']  # [B, 1, n_frames, H, W]\n",
    "\n",
    "    # Combine inputs if needed (e.g., concatenate along channel dimension)\n",
    "    # Need to concatenate depends on model architecture!\n",
    "    # Example: Combine frames and modal_masks as input\n",
    "    inputs = torch.cat([frames, modal_masks.float()], dim=1)  # [B, 4, n_frames, H, W]\n",
    "\n",
    "    # Now you can pass `inputs` to the model and use `amodal_masks` as the target\n",
    "    output = model(inputs)\n",
    "    loss = loss_fn(output, amodal_masks)\n",
    "    loss.backward()\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscenv",
   "language": "python",
   "name": "dsc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
